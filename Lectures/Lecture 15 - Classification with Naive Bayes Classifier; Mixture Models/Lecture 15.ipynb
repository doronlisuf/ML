{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Details about Midterm Exam\n",
    "\n",
    "## Date & Time: <font color=blue>Thursday, October 7</font>\n",
    "\n",
    "Check your email for the time you are scheduled to take the midterm exam. Let me know if there are conflicts well in advance.\n",
    "\n",
    "* **All exams will be proctored with Honorlock.**\n",
    "\n",
    "## In-class review: <font color=orange>Monday, October 4</font>\n",
    "\n",
    "Post questions in the [Midterm Exam Discussion Board](https://ufl.instructure.com/courses/435230/discussion_topics/3175588) until **Sunday, October 1 @ 3 PM**.\n",
    "* These questions will be addressed in-class on Monday, October 4.\n",
    "\n",
    "## Coverage\n",
    "\n",
    "Lectures 1-17 (modules 1-4)\n",
    "\n",
    "\n",
    "## Practice Midterm Exam\n",
    "\n",
    "1. Available in the [Assignment-Solutions](https://github.com/Fundamentals-of-Machine-Learning-F21/Assignments-Solutions) repo.\n",
    "\n",
    "2. Practice Canvas Quizzes created to practice Honorlock proctoring and overall exam setup: [Pratice Midterm Exam - Honolorck Proctoring](https://ufl.instructure.com/courses/435230/quizzes/1036559)\n",
    "\n",
    "3. Solutions to the practice exam will be posted before the in-class midterm review.\n",
    "\n",
    "4. One of the questions in the practice exam asks about K-Means clustering, but K-Means clustering will __not__ be covered in the Midterm exam.\n",
    "\n",
    "## Allowed Material for the Midterm\n",
    "* 1-page letter-sized of **formulas** (front and back, handwritten or typed)\n",
    "* scientific calculator\n",
    "* Writing and scratch paper to write your answers. I will give an estimate on how many pages to bring in the review lecture.\n",
    "* Bring a second device (e.g., phone, tablet or other) with the [CamScanner](https://www.camscanner.com/user/download) or [Scannable](https://evernote.com/products/scannable) app installed. This device is only to be used at the end of the exam to take pictures of your handwritten solutions\n",
    "* **<font color=blue>TOTAL TIME:</font>** 2 hours + 15 minutes\n",
    " \n",
    "**<font color=red>Communications between students or anyone else during the exam is considered cheating. Turn off all Slack notifications and other communications channels!</font>**\n",
    "\n",
    "Ten (10) minutes prior to the time you are scheduled to take the exam, you will be able to see the Canvas quiz with your midterm exam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 15 - Naive Bayes Classifier; Mixture Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier\n",
    "\n",
    "Therefore, for a given test point $\\mathbf{x}^*$, our decision rule is:\n",
    "\n",
    "$$P(C_1|\\mathbf{x}^*) \\underset{C_2}{\\overset{C_1}{\\gtrless}} P(C_2|\\mathbf{x}^*)$$\n",
    "\n",
    "Using the Bayes' rule, we can further rewrite it as:\n",
    "\\begin{align*}\n",
    "\\frac{P(\\mathbf{x}^*|C_1)P(C_1)}{P(\\mathbf{x}^*)} &\\underset{C_2}{\\overset{C_1}{\\gtrless}} \\frac{P(\\mathbf{x}^*|C_2)P(C_2)}{P(\\mathbf{x}^*)}\n",
    "\\end{align*}\n",
    "\n",
    "This defines the **Naive Bayes Classifier**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Generative Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, **to train the classifier**, what we need to do is to determine the parametric form (and its parameters values) for $p(x|C_1)$, $p(x|C_2)$, $P(C_1)$ and $p(C_2)$.\n",
    "\n",
    "* For example, we can assume that the data samples coming from either $C_1$ and $C_2$ are distributed according to Gaussian distributions. In this case, \n",
    "\n",
    "$$p(x|C_k) = \\frac{1}{(2\\pi)^{1/2} |\\Sigma_k|^{1/2}}\\exp\\left\\{-\\frac{1}{2}(\\mathbf{x}-\\mathbf{\\mu}_k)^T\\Sigma_k^{-1}(\\mathbf{x}-\\mathbf{\\mu}_k)\\right\\}, \\forall k=\\{1,2\\}$$\n",
    "\n",
    "For this case, this defines the **Gaussian Naive Bayes Classifier**.\n",
    "\n",
    "Or, we can consider any other parametric distribution.\n",
    "\n",
    "* What about the $P(C_1)$ and $P(C_2)$?\n",
    "\n",
    "We can consider the relative frequency of each class, that is, $P(C_k) = \\frac{N_k}{N}$, where $N_k$ is the number of points in class $C_k$ and $N$ is the total number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE Parameter Estimation Steps\n",
    "\n",
    "For simplification, let's consider the covariance matrix $\\Sigma_k$ for $k=1,2$ to be **isotropic** matrices, that is, the covariance matrix is diagonal and the element along the diagonal is the same, or: $\\Sigma_k = \\sigma_k^2\\mathbf{I}$.\n",
    "\n",
    "* What are the parameters? The mean and covariance of the Gaussian distribution for both classes.\n",
    "\n",
    "Given the assumption of the Gaussian form, how would you estimate the parameters for $p(x|C_1)$ and $p(x|C_2)$? We can use **maximum likelihood estimate** for the mean and covariance, because we are looking for the parameters of the distributions that *maximize* the data likelihood!\n",
    "\n",
    "The MLE estimate for the mean of class $C_k$ is:\n",
    "\n",
    "$$\\mathbf{\\mu}_{k,\\text{MLE}} = \\frac{1}{N_k}\\sum_{n\\in C_k} \\mathbf{x}_n$$\n",
    "\n",
    "where $N_k$ is the number of training data points that belong to class $C_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming the classes follow a (bivariate or 2-D) Gaussian distribution and, for simplicity, let's assume the covariance matrices are **isotropic**, that is, $\\Sigma_k = \\sigma^2_k \\mathbf{I}$.\n",
    "\n",
    "The MLE steps for parameter estimation are:\n",
    "\n",
    "1. Write down the observed data likelihood, $\\mathcal{L}^0$\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L}^0 &= P(x_1,x_2,\\dots,x_N|C_k)\\\\\n",
    "&= \\prod_{n=1}^N P(x_n|C_k),\\text{ data samples are i.i.d.} \\\\\n",
    "&= \\prod_{n=1}^N \\frac{1}{(2\\pi)^{1/2} |\\Sigma|^{1/2}} \\exp\\left\\{-\\frac{1}{2}(x_n-\\mu_k)^T\\Sigma_k^{-1}(x_n-\\mu_k)\\right\\}\\\\\n",
    "&= \\prod_{n=1}^N \\frac{1}{(2\\pi)^{1/2} |\\sigma_k^2 \\mathbf{I}|^{1/2}} \\exp\\left\\{-\\frac{1}{2\\sigma_k^2}(x_n-\\mu_k)^T\\mathbf{I}(x_n-\\mu_k)\\right\\}\\\\\n",
    "&= \\prod_{n=1}^N \\frac{1}{(2\\pi)^{1/2} (\\sigma_k^2)^{1/2}} \\exp\\left\\{-\\frac{1}{2\\sigma_k^2}(x_n-\\mu_k)^T(x_n-\\mu_k)\\right\\}\n",
    "\\end{align}\n",
    "\n",
    "2. Take the log-likelihood, $\\mathbf{L}$. This *trick* helps in taking derivatives.\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L} &= \\ln\\left(\\mathcal{L}^0\\right) \\\\\n",
    "&= \\sum_{n=1}^N -\\frac{1}{2}\\ln 2\\pi - \\frac{1}{2}\\ln\\sigma_k^2 - \\frac{1}{2\\sigma_k^2}(x_n-\\mu_k)^T(x_n-\\mu_k)\n",
    "\\end{align}\n",
    "\n",
    "3. Take the derivative of the log-likelihood function with respect to the parameters of interest. For Gaussian distribution they are the mean and covariance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mu_k} &= 0\\\\\n",
    "\\sum_{n\\in C_k} \\frac{1}{\\sigma_k^2} (x_n - \\mu_k) &= 0\\\\\n",
    "\\sum_{n\\in C_k} (x_n - \\mu_k) &= 0 \\\\\n",
    "\\sum_{n\\in C_k} x_n - \\sum_{n\\in C_k} \\mu_k &= 0 \\\\\n",
    "\\sum_{n\\in C_k} x_n - N_k \\mu_k &= 0 \\\\\n",
    "\\mu_k & = \\frac{1}{N_k} \\sum_{n\\in C_k} x_n\n",
    "\\end{align}\n",
    "\n",
    "This is the sample mean for each class. And,\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\sigma_k^2} &= 0\\\\\n",
    "\\sum_{n\\in C_k} -\\frac{1}{2\\sigma_k^2} + \\frac{2(x_n - \\mu_k)^T(x_n - \\mu_k)}{(2\\sigma_k^2)^2} &=0 \\\\\n",
    "\\sum_{n\\in C_k} -1 + \\frac{(x_n - \\mu_k)^T(x_n - \\mu_k)}{\\sigma_k^2} &=0 \\\\\n",
    "\\sum_{n\\in C_k} \\frac{(x_n - \\mu_k)^T(x_n - \\mu_k)}{\\sigma_k^2} &=N_k \\\\\n",
    "\\sigma_k^2 &= \\sum_{n\\in C_k} \\frac{(x_n - \\mu_k)^T(x_n - \\mu_k)}{N_k}\n",
    "\\end{align}\n",
    "\n",
    "This is the sample variance for each class. Then we can create $\\Sigma_k = \\sigma_k^2 \\mathbf{I}$, which is the (biased) sample covariance for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, if we want to estimate an entire covariance matrix, we would have to take the derivative of the log-likelihood function with respect to every entry in the covariance matrix.\n",
    "\n",
    "We can determine the values for $p(C_1)$ and $p(C_2)$ from the number of data points in each class:\n",
    "\n",
    "$$p(C_k) = \\frac{N_k}{N}$$\n",
    "\n",
    "where $N$ is the total number of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('bmh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateData(mean1, mean2, cov1, cov2, N1, N2):\n",
    "    # We are generating data from two Gaussians to represent two classes\n",
    "    # In practice, we would not do this - we would just have data from the problem we are trying to understand\n",
    "    data_C1 = np.random.multivariate_normal(mean1, cov1, N1)\n",
    "    data_C2 = np.random.multivariate_normal(mean2, cov2, N2)\n",
    "    \n",
    "    plt.scatter(data_C1[:,0], data_C1[:,1], c='r')\n",
    "    plt.scatter(data_C2[:,0], data_C2[:,1])\n",
    "    plt.xlabel('Feature 1'); plt.ylabel('Feature 2')\n",
    "    plt.axis([-4,4,-4,4])\n",
    "    return data_C1, data_C2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean1 = [-1, -1]\n",
    "mean2 = [1, 1]\n",
    "cov1 = [[1,0],[0,1]]\n",
    "cov2 = [[1,0],[0,1]]\n",
    "N1 = 50\n",
    "N2 = 100\n",
    "\n",
    "data_C1, data_C2 = generateData(mean1, mean2, cov1, cov2, N1, N2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the mean and covariance for each class from the training data\n",
    "\n",
    "\n",
    "print('Mean of Class 1: ', mu1)\n",
    "\n",
    "\n",
    "\n",
    "print('Covariance of Class 1: ',cov1)\n",
    "\n",
    "\n",
    "\n",
    "print('Mean of Class 2: ',mu2)\n",
    "\n",
    "\n",
    "\n",
    "print('Covariance of Class 2: ',cov2)\n",
    "\n",
    "\n",
    "\n",
    "# Estimate the prior for each class\n",
    "\n",
    "\n",
    "print('Probability of  Class 1: ',pC1)\n",
    "\n",
    "\n",
    "\n",
    "print('Probability of Class 2: ',pC2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute a grid of values for x and y \n",
    "x = np.linspace(-4, 4, 100)\n",
    "y = np.linspace(-4, 4, 100)\n",
    "xm, ym = np.meshgrid(x, y)\n",
    "X = np.flip(np.dstack([xm,ym]),axis=0)\n",
    "\n",
    "# Let's plot the probabaility density function (pdf) for each class\n",
    "\n",
    "# TO BE FINISHED IN CLASS\n",
    "\n",
    "\n",
    "fig =plt.figure(figsize=(15,5))\n",
    "fig.add_subplot(1,2,1)\n",
    "plt.imshow(y1, extent=[-4,4,-4,4])\n",
    "plt.colorbar()\n",
    "plt.xlabel('Feature 1'); plt.ylabel('Feature 2')\n",
    "plt.title('PDF Likelihood for Class 1')\n",
    "\n",
    "fig.add_subplot(1,2,2)\n",
    "plt.imshow(y2, extent=[-4,4,-4,4])\n",
    "plt.colorbar()\n",
    "plt.xlabel('Feature 1'); plt.ylabel('Feature 2')\n",
    "plt.title('PDF Likelihood for Class 2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig =plt.figure(figsize=(15,5))\n",
    "fig.add_subplot(1,2,1)\n",
    "plt.scatter(data_C1[:,0], data_C1[:,1], c='r',alpha=0.3)\n",
    "plt.imshow(y1, extent=[-4,4,-4,4],cmap='YlOrRd')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Feature 1'); plt.ylabel('Feature 2')\n",
    "plt.title('PDF Likelihood for Class 1')\n",
    "\n",
    "fig.add_subplot(1,2,2)\n",
    "plt.scatter(data_C2[:,0], data_C2[:,1], c='b',alpha=0.3)\n",
    "plt.imshow(y2, extent=[-4,4,-4,4], cmap='Blues')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Feature 1'); plt.ylabel('Feature 2')\n",
    "plt.title('PDF Likelihood for Class 2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the posterior distributions: they represent our classification decision\n",
    "\n",
    "# TO BE FINISHED IN CLASS\n",
    "\n",
    "fig =plt.figure(figsize=(15,5))\n",
    "fig.add_subplot(1,2,1)\n",
    "plt.imshow(pos1, extent=[-4,4,-4,4])\n",
    "plt.colorbar()\n",
    "plt.xlabel('Feature 1'); plt.ylabel('Feature 2')\n",
    "plt.title('Posterior for Class 1')\n",
    "\n",
    "fig.add_subplot(1,2,2)\n",
    "plt.imshow(pos2, extent=[-4,4,-4,4])\n",
    "plt.colorbar()\n",
    "plt.xlabel('Feature 1'); plt.ylabel('Feature 2')\n",
    "plt.title('Posterior for Class 2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the decision boundary:\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.imshow(pos1 > pos2, extent=[-4,4,-4,4])\n",
    "plt.colorbar()\n",
    "plt.xlabel('Feature 1'); plt.ylabel('Feature 2')\n",
    "plt.title('Region to Decide Class 1');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's use this classifier to predict the class label for point $[1,1]$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1,1]\n",
    "\n",
    "# Data Likelihoods\n",
    "# TO BE FINISHED IN CLASS\n",
    "\n",
    "print('Data likelihoods:')\n",
    "print('P(x|C1) = ', y1_newPoint)\n",
    "print('P(x|C2) = ', y2_newPoint,'\\n')\n",
    "\n",
    "# Posterior Probabilities\n",
    "# TO BE FINISHED IN CLASS\n",
    "\n",
    "print('Posterior probabilities:')\n",
    "print('P(C1|x) = ', y1_pos)\n",
    "print('P(C2|x) = ', y2_pos,'\\n')\n",
    "\n",
    "# TO BE FINISHED IN CLASS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What about $x=[2,4]$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixture Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we look at a relatively simple model where we model each class with a single Gaussian probability density function (pdf).\n",
    "\n",
    "* What if the data for a single class looked like the plot below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "data, _ = make_blobs(n_samples = 1500, centers = 5)\n",
    "\n",
    "plt.scatter(data[:,0],data[:,1]); plt.axis([-15,15,-15,15])\n",
    "plt.xlabel('Feature 1'); plt.ylabel('Feature 2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume a single Gaussian distribution, we would obtain a very poor estimate of the true underlying data likelihood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute a grid of values for x and y \n",
    "x = np.linspace(-15, 15, 100)\n",
    "y = np.linspace(-15,15, 100)\n",
    "xm, ym = np.meshgrid(x, y)\n",
    "X = np.flip(np.dstack([xm,ym]),axis=0)\n",
    "\n",
    "y1 = multivariate_normal.pdf(X, mean=np.mean(data, axis=0), cov=np.cov(data.T))\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(y1, extent=[-15,15,15,-15])\n",
    "plt.xlabel('Feature 1'); plt.ylabel('Feature 2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixture Models\n",
    "\n",
    "We can better represent this data with a **mixture model**:\n",
    "\n",
    "$$p(x|\\Theta) = \\sum_{k=1}^K \\pi_k P(x|\\Theta_k)$$\n",
    "\n",
    "where $\\Theta = \\{\\Theta_k\\}_{k=1}^K$ are set of parameters that define the distributional form in the probabilistic model $P(\\bullet|\\Theta_k)$ and \n",
    "\n",
    "\\begin{align}\n",
    "0 & \\leq \\pi_k \\leq 1\\\\\n",
    "& \\sum_k \\pi_k = 1\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the probabilistic model $P(\\bullet|\\Theta_k)$ is assumed to be a Gaussian distribution, then the above mixture model is a **Gaussian Mixture Model (GMM)**\n",
    "\n",
    "$$P(x|\\Theta) = \\sum_{k=1}^K \\pi_k \\mathcal{N}(x|\\mu_k, \\Sigma_k)$$\n",
    "\n",
    "where $\\Theta_k=\\{\\mu_k, \\Sigma_k, \\pi_k\\}, \\forall k$ are the mean, covariance and weight of each Gaussian distribution, and, again, $0 \\leq \\pi_k \\leq 1$ with $\\sum_k \\pi_k = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating Gaussian Mixture Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=blue>Exercise:</font> In code, how would you draw a sample from a Gaussian Mixture Model? Or from a mixture model in general?**\n",
    "\n",
    "* Note that in a Gaussian Mixture Model we are assuming that each data point $x_i$ was drawn from only one Gaussian. \n",
    "\n",
    "* Each data point $x_i$ has a *hard membership*.\n",
    "\n",
    "* Each Gaussian in the Mixture Model will have its own $0 \\leq \\pi_k \\leq 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simulate an event with arbitary probability $P_E$:\n",
    "\n",
    "1. Generate a random number R that is equally likely to be between 0 and 1.\n",
    "\n",
    "2. If $R \\le P_E$, then in the simulation, the event occurs. Otherwise it does not occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.random as npr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?npr.uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example simulation\n",
    "Pe=0.13\n",
    "\n",
    "num_sims=100_000\n",
    "event_count=0\n",
    "\n",
    "# TO BE FINISHED IN CLASS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the case where we have 4 Gaussian in the Mixture model with weights $[0.4,0.25,0.25,0.1]$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pis = [.4, .25, .25, .1]\n",
    "\n",
    "\n",
    "plt.xticks(range(1,5),['$G_1$','$G_2$','$G_3$','$G_4$'])\n",
    "plt.ylabel('Prior Probability \\nfor each Gaussian component');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to first (randomly) select a Gaussian and then draw a point from it.\n",
    "\n",
    "* How do you select from this set of Gaussians?\n",
    "\n",
    "* We can sum up the $\\pi$'s as we move from left to right and plot the running sums (or cumulative sum), then we can sample a Gaussian using a Uniform random number generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.xticks(range(1,5),['$G_1$','$G_2$','$G_3$','$G_4$'])\n",
    "plt.ylabel('Cumulative Probability');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_GaussianMixture(N, Means, Sigs, Pis):\n",
    "    X = np.empty((0,Means.shape[1]))\n",
    "    L = np.empty(0)\n",
    "    for i in range(N):\n",
    "        rv = npr.uniform()   # sample uniform RV\n",
    "        GaussianChosen = np.where(rv <= np.cumsum(Pis))[0][0]\n",
    "        L = np.append(L, GaussianChosen)\n",
    "        X = np.append(X, np.array([npr.multivariate_normal(Means[GaussianChosen], \n",
    "                                                           np.eye(Means.shape[1])*Sigs[GaussianChosen])]), axis=0)\n",
    "    return X, L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10_000\n",
    "Means = np.array([[1,1],[0,0],[-.5, 1],[.5, 1.5]])\n",
    "Sigs = [.1, .01, .05, .05]\n",
    "Pis = [.4, .25, .25, .1]\n",
    "\n",
    "X,L = make_GaussianMixture(N, Means, Sigs, Pis)\n",
    "\n",
    "fig = plt.figure(figsize=(14,6))\n",
    "fig.add_subplot(1,2,1)\n",
    "plt.scatter(X[:,0],X[:,1], c=L)\n",
    "plt.title('Synthetic Data with known Gaussians');\n",
    "fig.add_subplot(1,2,2)\n",
    "plt.scatter(X[:,0],X[:,1])\n",
    "plt.title('Gaussian Mixture - as received from the world');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Likelihood as a Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GMMs can be used to learn a complex distribution that represent a dataset. Thus, it can be used within the probabilistic generative classifier framework to model complex data likelihoods.\n",
    "\n",
    "* GMMs are also commonly used for **clustering**. Here a GMM is fit to a dataset with the goal of partitioning it into clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=blue>Step 1</font>**\n",
    "\n",
    "We can write the **observed data likelihood**, $\\mathcal{L}^o$, as:\n",
    "\n",
    "$$\\mathcal{L}^0 = \\prod_{i=1}^N p(x_i|\\Theta)$$\n",
    "\n",
    "where\n",
    "\n",
    "$$p(x_i|\\Theta) = \\sum_{k=1}^K \\pi_k N(x_i|\\mu_k, \\Sigma_k)$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\Theta = \\{\\pi_k, \\mu_k, \\Sigma_k\\}_{k=1}^K$$\n",
    "\n",
    "Therefore, \n",
    "\n",
    "$$\\mathcal{L}^0 = \\prod_{i=1}^N \\sum_{k=1}^K \\pi_k \\mathcal{N}(x_i|\\mu_k, \\Sigma_k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=blue>Step 2</font>**\n",
    "\n",
    "Let's apply our *trick*: log-likelihood.\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L} &= \\ln\\left(\\prod_{i=1}^N \\sum_{k=1}^K \\pi_k \\mathcal{N}(x_i|\\mu_k, \\Sigma_k)\\right)\\\\\n",
    "&= \\sum_{i=1}^N \\ln \\left( \\sum_{k=1}^K \\pi_k \\mathcal{N}(x_i|\\mu_k, \\Sigma_k) \\right)\n",
    "\\end{align}\n",
    "\n",
    "The \"trick\" does not work as well as before... We are stuck with a term that is the log of sums, which cannot be decomposed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=blue>Step 3</font>**\n",
    "\n",
    "Optimize for the parameters.\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mu_k} = 0, \\frac{\\partial \\mathcal{L}}{\\partial \\Sigma_k} = 0, \\text{ and }, \\frac{\\partial \\mathcal{L}}{\\partial \\pi_k} = 0$$\n",
    "\n",
    "\n",
    "but this is a difficult problem to maximize!\n",
    "\n",
    "* A common approach for estimating the parameters of a GMM given a data set is by using the **Expectation-Maximization (EM) algorithm**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications of (Gaussian) Mixture Models\n",
    "\n",
    "* **Data Representation and Inference**. Represent *any* data set using a complex distribution. You may want to describe your data with a distribution but the data does not fit into a standard form (e.g. Gamma, Gaussian, Exponential, etc.), then you can use a (Gaussian) Mixture Model to describe the data. Having a representation of your data in a distribution form is a very powerful tool that, other than having a model of the data, allows you infer about new samples, make predictions, etc.\n",
    "    * Having a parametric form for a real world model, allows us to apply it in simulation and use it for designing/optimizing decision-making solutions.\n",
    "\n",
    "* **Clustering.** Partition the data into groups. Note that in the GMMs formulation we did not add the concept of labels/targets. So GMMs are an **unsupervised learning** model. It represents the data with a very complex likelihood and then we can decompose that likelihood to partition the data into categories. This is also known as **clustering**.\n",
    "    * Commonly used in financial institutions to learn different group of individuals with specific characteristics of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectation-Maximization (EM) algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the observation of i.i.d. samples $x_1, x_2, \\dots, x_N$ from the data likelihood $p(\\mathbf{x}|\\Theta)$ and we want to estimate the parameters (using MLE approach, for example)\n",
    "\n",
    "\\begin{align}\n",
    "& \\arg_{\\Theta} \\max p(\\mathbf{x}|\\Theta) \\\\\n",
    "= & \\arg_{\\Theta} \\max \\prod_{i=1}^N p(x_i|\\Theta) \n",
    "\\end{align}\n",
    "\n",
    "Now suppose the data samples $x_1, x_2, \\dots, x_N$ are **censored**.\n",
    "\n",
    "For example, suppose we observe i.i.d. samples, $x_1, x_2, \\dots, x_N$, from some sensor $f(\\mathbf{x})$. This sensor returns censored data in the form,\n",
    "\n",
    "\\begin{align}\n",
    "f(\\mathbf{x}) = \\begin{cases} x, &&\\text{ if }  x < a \\\\ a && \\text{ if }x\\geq a \\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "This means that we see $x_1, x_2,\\dots, x_m$ (less than $a$) and we do not see $x_{m+1}, x_{m+2}, \\dots, x_N$ which are censored and set to $a$.\n",
    "\n",
    "* An example of such censored data would be a scale that can only measure weight up to 120 lbs. Any measurements above 120 would be *censored* at 120.\n",
    "\n",
    "* Given this censored data, we want to estimate the mean of the data as if the data was uncensored.\n",
    "\n",
    "For this case, we can write our *observed* data likelihood as:\n",
    "\n",
    "$$\\mathcal{L}^0 = \\prod_{i=1}^m p(x_i|\\Theta) \\prod_{j=m+1}^N \\int_a^{\\infty} p(x_j|\\Theta) dx_j$$\n",
    "\n",
    "The data likelihood would be very difficult to maximize to solve for $\\Theta$.\n",
    "\n",
    "If only we *knew* what the missing/censored data, the problem would be easy to solve!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
