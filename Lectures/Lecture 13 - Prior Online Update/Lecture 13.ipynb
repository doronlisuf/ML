{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 13 - Prior Online Update; Introduction to Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review of Gaussian Distribution\n",
    "\n",
    "(Read [section 2.3 \"The Gaussian Distribution\"](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf) from the Bishop textbook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Gaussian\n",
    "\n",
    "The Gaussian distribution is a widely used probabilistic model for the probability density function (pdf) of continuous random variables. \n",
    "\n",
    "The Gaussian distribution can model both univariate (1-D) or multivariate (multi-dimensional) samples.\n",
    "\n",
    "In the **univariate** case, the pdf of a Gaussian distribution for a random variable $X\\in\\mathbb{R}$ can be written as\n",
    "\n",
    "$$f_X(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{\\left(x-\\mu\\right)^2}{2\\sigma^2}\\right)$$\n",
    "\n",
    "In this case, we say that $X$ follows a Gaussian distribution with mean $\\mu$ and variance $\\sigma^2$, or, $X\\sim N(\\mu,\\sigma^2)$.\n",
    "\n",
    "* We can define the **precision** parameter $\\beta$ as the inverse of the variance, that is, $\\beta=\\frac{1}{\\sigma^2}$.\n",
    "\n",
    "* A Gaussian distribution is called **Normal** when the mean is $\\mu=0$ and variance is $\\sigma^2=1$, $X\\sim N(0,1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G1=stats.norm(0,1)\n",
    "\n",
    "G2=stats.norm(10,3)\n",
    "\n",
    "G3=stats.norm(-5,0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "x=np.linspace(-8,18,1000)\n",
    "plt.plot(x,G1.pdf(x),label='Gaussian(0,1)')\n",
    "plt.plot(x,G2.pdf(x),'--',label='Gaussian(10,9)')\n",
    "plt.plot(x,G3.pdf(x),'-.',label='Gaussian(-5,0.09)')\n",
    "plt.legend(fontsize=15)\n",
    "plt.xlabel('$x$',size=15)\n",
    "plt.ylabel('Probability Density Function \\n $f_X(x)$',size=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples=G1.rvs(size=100)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "x=np.linspace(-5,5,100)\n",
    "plt.hist(samples,density=True, label='Histogram for\\n 100 samples')\n",
    "plt.plot(x, G1.pdf(x), label='pdf for G(0,1)')\n",
    "plt.legend(fontsize=15)\n",
    "plt.xlabel('$x$',size=15)\n",
    "plt.ylabel('Density',size=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Gaussian\n",
    "\n",
    "In the **multivariate** case, the pdf of a Gaussian distribution for a random variable $X\\in\\mathbb{R}^D$ can be written as\n",
    "\n",
    "$$f_X(x) = \\frac{1}{(2\\pi)^{1/2}\\left|\\Sigma\\right|^{d/2}}\\exp\\left(-\\frac{1}{2}\\left(\\mathbf{x}-\\mathbf{\\mu}\\right)^T\\Sigma^{-1}\\left(\\mathbf{x}-\\mathbf{\\mu}\\right)\\right)$$\n",
    "\n",
    "In this case, we say that $X$ follows a Gaussian distribution with mean $\\mu$ and covariance $\\Sigma$, or, $X\\sim N(\\mu,\\Sigma)$.\n",
    "\n",
    "* $\\mu$ is a $D$-dimensional mean vector\n",
    "* $\\Sigma$ is a $D\\times D$ covariance matrix\n",
    "* $\\left|\\Sigma\\right|$ denotes the determinant of $\\Sigma$\n",
    "* The precision parameter in a $D$-dimensional space is equal to $\\beta = \\Sigma^{-1}$\n",
    "\n",
    "Let $X=[X_1,X_2]$. The **covariance** $\\Sigma$ measures the amount of variance is each individual dimension, $X_1$ and $X_2$, as well as the amount of covariance between the two. We can write the covariance as\n",
    "\n",
    "\\begin{align}\n",
    "\\text{cov}(X_1,X_2) &= E\\bigl[\\left(X_1-E\\left[X_1\\right]\\right) \\left(X_2-E\\left[X_2\\right]\\right)\\bigr]\\\\\n",
    "&= \\left[\\begin{array}{cc}\\text{var}(X_1) & \\text{cov}(X_1,X_2) \\\\ \\text{cov}(X_1,X_2) & \\text{var}(X_2)\\end{array}\\right]\\\\\n",
    "&= \\left[\\begin{array}{cc}\\sigma^2_{X_1} & \\text{cov}(X_1,X_2) \\\\ \\text{cov}(X_1,X_2) & \\sigma^2_{X_2}\\end{array}\\right]\n",
    "\\end{align}\n",
    "\n",
    "* The **Pearson's correlation coefficient** between random variables $X_1$ and $X_2$ is defined as:\n",
    "\n",
    "$$ r = \\frac{\\operatorname{cov}(X_1,X_2)}{\\sqrt{\\text{var}(X_1)}\\sqrt{\\text{var}(X_2)}} = \\frac{\\text{cov}(X_1,X_2)}{\\sigma_{X_1} \\sigma_{X_2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = np.array([0,0])\n",
    "covariance  = np.array([[1,0],[0,1]])\n",
    "\n",
    "G = stats.multivariate_normal(mean, cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.mgrid[-6:6:100j, -6:6:100j]\n",
    "xy = np.column_stack([x.flat, y.flat])\n",
    "z = stats.multivariate_normal.pdf(xy, mean=mu, cov=covariance)\n",
    "z = z.reshape(x.shape)\n",
    "\n",
    "%matplotlib notebook\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(x,y,z, rstride=3, cstride=3, linewidth=1, antialiased=True,\n",
    "                cmap=plt.cm.viridis)\n",
    "ax.set_xlabel('$x_1$',size=15)\n",
    "ax.set_ylabel('$x_2$',size=15)\n",
    "ax.set_zlabel('PDF $f_X(x_1,x_2)$',size=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the Mathematica's demonstration [\"Joint Density of Bivariate Gaussian Random Variables\"](https://demonstrations.wolfram.com/JointDensityOfBivariateGaussianRandomVariables/) to better understand the role of the covariance matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prior Online Update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Gaussian-Gaussian Conjugate Prior Relationship\n",
    "\n",
    "For a D-dimensional Gaussian data likelihood with mean $\\mu$ and covariance $\\beta\\mathbf{I}$ and a prior distribution with mean $\\mu_0$ and covariance $\\Sigma_0$\n",
    "\n",
    "\\begin{align}\n",
    "P(\\mathbf{t}|\\mathbf{w}) &\\sim \\mathcal{N}(\\mathbf{\\mu}, \\beta\\mathbf{I}) \\\\\n",
    "P(\\mathbf{w}) &\\sim \\mathcal{N}(\\mathbf{\\mu}_0,\\Sigma_0)\n",
    "\\end{align}\n",
    "\n",
    "The posterior distribution\n",
    "\n",
    "\\begin{align}\n",
    "P(\\mathbf{w}|\\mathbf{t}) &\\sim \\mathcal{N}\\left(\\mathbf{\\mu}_N, \\Sigma_N\\right) \\\\\n",
    "\\mathbf{\\mu}_N &= \\Sigma_N \\left(\\Sigma_0^{-1}\\mathbf{\\mu}_0+\\beta\\mathbf{\\mathbf{X}}^T\\mathbf{t}\\right)\\\\\n",
    "\\Sigma_N^{-1} &= \\Sigma_0^{-1} + \\beta \\mathbf{\\mathbf{X}}^T\\mathbf{\\mathbf{X}}\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathbf{X}$ is the feature matrix of size $N \\times M$.\n",
    "\n",
    "* What happens with different values of $\\beta$ and $\\Sigma_0$?\n",
    "\n",
    "To simplify, let's assume the covariance of prior is **isotropic**, that is, it is a diagonal matrix with the same value along the diagonal, $\\Sigma_0 = \\alpha^{-1}\\mathbf{I}$. And, let also $\\mathbf{\\mu}_0 = [0,0]$, thus \n",
    "\n",
    "$$\\mu_N = \\beta \\Sigma_N\\mathbf{X}^T\\mathbf{t}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\Sigma_N = \\left(\\alpha^{-1}\\mathbf{I} + \\beta \\mathbf{X}^T\\mathbf{X}\\right)^{-1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Online Updating of the Prior using Conjugate Priors (Gaussian-Gaussian)\n",
    "\n",
    "Let's consider the example presented in the Bishop textbook (Figure 3.7 in page 155).\n",
    "\n",
    "Consider a single input variable $\\mathbf{x}$, a single target variable $\\mathbf{t}$ and a linear model of the form $y(\\mathbf{x},\\mathbf{w}) = w_0 + w_1\\mathbf{x}$.\n",
    "Because this has just two parameters coefficients, $w=[w_0, w_1]^T$, we can plot the prior and posterior distributions directly in parameter space (2-dimensional parameter space)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "Let's generate some synthetic data from the function $f(x, a) = w_0 + w_1x$ with parameter values $w_0 = −0.3$ and $w_1 = 0.5$ by first choosing values of $x_n$ from the uniform distribution $U(x_n|−1, 1)$, then evaluating $f(x_n, \\mathbf{w})$, and finally adding Gaussian noise with standard deviation of $\\sigma = 0.2$ to obtain the target values $t_n$.\n",
    "\n",
    "$$t_n = f(x_n, \\mathbf{w}) + \\epsilon = -0.3 + 0.5 x_n + \\mathbf{\\epsilon}$$\n",
    "\n",
    "where $\\mathbf{\\epsilon}\\sim \\mathcal{N}(0,\\beta\\mathbf{I})$.\n",
    "\n",
    "* **Our goal is to recover the values of $w_0$ and $w_1$ from such data, and we will explore the dependence on the size of the data set.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "For some data, $\\{x_n,t_n\\}_{n=1}^N$, we can pose this problem in terms of **Regularized Least Squares**:\n",
    "\n",
    "\\begin{align}\n",
    "J(\\mathbf{w}) &= \\frac{1}{2} \\sum_{n=1}^N \\left(t_n - y_n\\right)^2 + \\frac{\\lambda}{2} \\sum_{i=0}^1 w_i^2 \\\\\n",
    "&= \\frac{1}{N} \\sum_{n=1}^2 \\left(t_n - y_n\\right)^2 + \\frac{\\lambda}{2} \\left(w_0^2 + w_1^2\\right)\\\\\n",
    "& \\Rightarrow \\arg_{\\mathbf{w}}\\min J(\\mathbf{w})\n",
    "\\end{align}\n",
    "\n",
    "* Using **MAP**, we can rewrite our objective using the **Bayesian interpretation**:\n",
    "\n",
    "\\begin{align}\n",
    "\\arg_{\\mathbf{w}} \\max P(\\mathbf{\\epsilon}|\\mathbf{w})P(\\mathbf{w})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the data likelihood, $P(\\mathbf{\\epsilon}|\\mathbf{w})$, to be a Gaussian distribution with mean $\\mu = 0$ and variance $\\sigma^2 = \\beta\\mathbf{I}$. And let's also consider the prior distribution, $P(\\mathbf{w})$, to be a Gaussian distribution with mean $\\mu_0$ and variance $\\sigma_0^2 = \\alpha^{-1}\\mathbf{I}$. Then, using the derivations from above, we can rewrite our optimization as:\n",
    "\n",
    "\\begin{align}\n",
    "\\arg_{\\mathbf{w}} \\max & \\mathcal{N}(\\mathbf{\\epsilon}|0,\\beta\\mathbf{I})\\mathcal{N}(\\mathbf{w}|\\mathbf{\\mu_0} ,\\alpha^{-1}\\mathbf{I}) \\\\\n",
    "\\propto\\arg_{\\mathbf{w}} \\max & \\mathcal{N}\\left( \\beta \\Sigma_N^{-1} \\mathbf{X}^T\\mathbf{t}, \\Sigma_N \\right)\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathbf{\\mu}_0 = [0,0]$, $\\mathbf{X}$ is the polynomial feature matrix, and $\\Sigma_N = \\left(\\alpha^{-1}\\mathbf{I} + \\beta \\mathbf{X}^T \\mathbf{X}\\right)^{-1}$ is the covariance matrix of the posterior distribution.\n",
    "\n",
    "Note that we **do not known** the parameters of the prior distribution ($\\mu_0$ and $\\sigma_0$ are unknown). The parameters of the prior distribution will have to be chosen by the user. And they will essentially *encode* any behavior or a priori knowledge we may have about the weights.\n",
    "\n",
    "* **Both our data likelihood and prior distributions are in a 2-dimensional space (this is because our *model order* is $M=2$ -- we have 2 parameters!).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to generate data from $t = -0.3 + 0.5x + \\epsilon$ where $\\epsilon$ is drawn from a zero-mean Gaussin distribution.\n",
    "\n",
    "* **The goal is to estimate the values $w_0=-0.3$ and $w_1=0.5$**\n",
    "* The feature matrix $\\mathbf{X}$ can be computed using the polynomial basis functions\n",
    "* **Parameters to choose:** $\\beta$ and $\\alpha$\n",
    "\n",
    "We want to implement this scenario for a case that we are getting more data every minute. As we get more and more data, we want to **update our prior distribution using our posterior distribution (informative prior)**, because they take the have the same distribution form. This is only possible because because Gaussian-Gaussian have a conjugate prior relationship. That is, the posterior distribution is also a Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy.stats import multivariate_normal\n",
    "import textwrap\n",
    "\n",
    "def likelihood_prior_func(beta = 2, alpha = 1, draw_num=(0,1,10,20,50,100)):\n",
    "    '''Online Update of the Posterior distribution for a Gaussian-Gaussian conjugate prior.\n",
    "    Parameter:\n",
    "    beta - variance of the data likelihood (of the additive noise)\n",
    "    alpha - precision value or 1/variance of the prior distribution\n",
    "    draw_num - number of points collected at each instance.\n",
    "    \n",
    "    This function will update the prior distribution as new data points are received.\n",
    "    The prior distribution will be the posterior distribution from the last iteration.'''\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 20), dpi= 80, facecolor='w', edgecolor='k')\n",
    "\n",
    "    # true (unknown) weights\n",
    "    a = -0.3 # w0\n",
    "    b = 0.5  # w1\n",
    "    \n",
    "    # set up input space\n",
    "    rangeX = [-2, 2] # range of values for the input\n",
    "    step = 0.025 # distance between points\n",
    "    X = np.mgrid[rangeX[0]:rangeX[1]:step] # creates a grid of values for input samples\n",
    "\n",
    "    #initialize prior/posterior and sample data\n",
    "    S0 = (1/alpha)*np.eye(2) # prior covariance matrix\n",
    "    sigma = S0 # copying it so we can update it later\n",
    "    mean = [0,0] # mean for prior\n",
    "    \n",
    "    # Draws samples from Uniform(-1,1) distribution\n",
    "    draws = np.random.uniform(rangeX[0],rangeX[1],size=draw_num[-1])\n",
    "    # Generate the noisy target samples\n",
    "    T = a + b*draws + np.random.normal(loc=0, scale=math.sqrt(beta))\n",
    "\n",
    "    for i in range(len(draw_num)):\n",
    "        if draw_num[i]>0: #skip first image\n",
    "            \n",
    "            # INPUT DATA\n",
    "            #Feature Matrix (Polynomial features with M=2)\n",
    "            FeatureMatrix = np.array([draws[:draw_num[i]]**m for m in range(2)]).T\n",
    "            #Target Values\n",
    "            t = T[0:draw_num[i]]\n",
    "            \n",
    "            # POSTERIOR PROBABILITY\n",
    "            # Covariance matrix\n",
    "            sigma = np.linalg.inv(S0 + beta*FeatureMatrix.T@FeatureMatrix)\n",
    "            # Mean vector\n",
    "            mean = beta*sigma@FeatureMatrix.T@t\n",
    "            \n",
    "            # PARAMETER SPACE\n",
    "            # create a meshgrid of possible values for w's\n",
    "            w0, w1 = np.mgrid[rangeX[0]:rangeX[1]:step, rangeX[0]:rangeX[1]:step]\n",
    "            \n",
    "            # Define the Gaussian distribution for data likelihood\n",
    "            p = multivariate_normal(mean=t[draw_num[i]-1], cov=beta)\n",
    "            # Initialize the PDF for data likelihood\n",
    "            out = np.empty(w0.shape)\n",
    "            # For each value (w0,w1), compute the PDF for all data samples\n",
    "            for j in range(len(w0)):\n",
    "                out[j] = p.pdf(w0[j]+w1[j]*draws[draw_num[i]-1])\n",
    "            \n",
    "            # Plot the data likelihood\n",
    "            ax = fig.add_subplot(*[len(draw_num),3,(i)*3+1])\n",
    "            ax.pcolor(w0, w1, out)\n",
    "            # Add the current value for parameters w=(w0,w1)\n",
    "            ax.scatter(a,b, c='c')\n",
    "            myTitle = 'data likelihood'\n",
    "            ax.set_title(\"\\n\".join(textwrap.wrap(myTitle, 100)))\n",
    "\n",
    "        # PARAMETER SPACE\n",
    "        # create a meshgrid of possible values for w's\n",
    "        w0, w1 = np.mgrid[rangeX[0]:rangeX[1]:step, rangeX[0]:rangeX[1]:step]\n",
    "        \n",
    "        # POSTERIOR PROBABILITY\n",
    "        # initialize the matrix with posterior PDF values\n",
    "        pos = np.empty(w1.shape + (2,))\n",
    "        # for w0\n",
    "        pos[:, :, 0] = w0\n",
    "        # and for w1\n",
    "        pos[:, :, 1] = w1\n",
    "        # compute the PDF\n",
    "        p = multivariate_normal(mean=mean, cov=sigma)\n",
    "\n",
    "        #Show prior/posterior\n",
    "        ax = fig.add_subplot(*[len(draw_num),3,(i)*3+2])\n",
    "        ax.pcolor(w0, w1, p.pdf(pos))\n",
    "        # Add the value for parameters w=(w0,w1) that MAXIMIZE THE POSTERIOR\n",
    "        ax.scatter(a,b, c='c')\n",
    "        myTitle = 'Prior/Posterior'\n",
    "        ax.set_title(\"\\n\".join(textwrap.wrap(myTitle, 100)))\n",
    "\n",
    "        # DATA SPACE\n",
    "        ax = fig.add_subplot(*[len(draw_num),3,(i)*3+3])\n",
    "        for j in range(6):\n",
    "            # draw sample from the prior probability to generate possible values for parameters \n",
    "            w0, w1 = np.random.multivariate_normal(mean=mean, cov=sigma)\n",
    "            # Estimated labels\n",
    "            t = w0 + w1*X\n",
    "            # Show data space\n",
    "            ax.plot(X,t)\n",
    "            if draw_num[i] > 0:\n",
    "                ax.scatter(FeatureMatrix[:,1], T[0:draw_num[i]])\n",
    "            myTitle = 'data space'\n",
    "            ax.set_title(\"\\n\".join(textwrap.wrap(myTitle, 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Parameters\n",
    "# beta - variance of the data likelihood (for the additive noise)\n",
    "# alpha - precision value or 1/variance for the prior distribution\n",
    "# draw_num - number of points collected at each instance\n",
    "\n",
    "likelihood_prior_func(beta = 0.5, alpha = 1/2, draw_num=(0,1,2,20,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Classification; Probabilistic Generative Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have focused on regression. We will begin to discuss **classification**.\n",
    "\n",
    "Suppose we have training data from two classes, $C_1$ and $C_2$, and we would like to train a classifier to assign a label to incoming test points whether they belong to class $C_1$ or $C_2$.\n",
    "\n",
    "There are *many* classifiers in the machine learning literature. We will cover a few in this course. Today we will focus on probabilistic generative approaches for classification.\n",
    "\n",
    "* There are two types of probabilistic models: **discriminative** and **generative**.\n",
    "\n",
    "A **discriminative** approach for classification is one in which we partition the feature space into regions for each class. Then, when we have a test point, we evaluate in which region it landed on and classify it accordingly.\n",
    "\n",
    "A **generative** approach for classification is one in which we estimate the parameters for distributions that generate the data for each class. Then, when we have a test point, we can compute the posterior probability of that point belonging to each class and assign the point to the class with the highest posterior probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateData(mean1, mean2, cov1, cov2, N1, N2):\n",
    "    # We are generating data from two Gaussians to represent two classes\n",
    "    # In practice, we would not do this - we would just have data from the problem we are trying to understand\n",
    "    class1X = np.random.multivariate_normal(mean1, cov1, N1)\n",
    "    class2X = np.random.multivariate_normal(mean2, cov2, N2)\n",
    "    \n",
    "    plt.scatter(class1X[:,0], class1X[:,1], c='r')\n",
    "    plt.scatter(class2X[:,0], class2X[:,1])\n",
    "    plt.xlabel('Feature 1'); plt.ylabel('Feature 2')\n",
    "    plt.axis([-4,4,-4,4])\n",
    "    return class1X, class2X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean1 = [-1, -1]\n",
    "mean2 = [1, 1]\n",
    "cov1 = [[1,0],[0,1]]\n",
    "cov2 = [[1,0],[0,1]]\n",
    "N1 = 50\n",
    "N2 = 100\n",
    "\n",
    "class1X, class2X = generateData(mean1, mean2, cov1, cov2, N1, N2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the data we generated above, we have a \"red\" class and a \"blue\" class. When we are given a test sample, we will want to assign the label of red or blue.\n",
    "\n",
    "We can compute the **posterior probability** for class $C_1$ as follows:\n",
    "\n",
    "$$P(C_1|x) = \\frac{P(x|C_1)P(C_1)}{P(x)}$$\n",
    "\n",
    "Understanding that the two classes, red and blue, form a partition of all possible classes, then we can utilize the *Law of Total Probability*, and obtain:\n",
    "\n",
    "$$P(C_1|x)=\\frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1) + P(x|C_2)P(C_2)}$$\n",
    "\n",
    "Similarly, we can compute the posterior probability for class $C_2$:\n",
    "\n",
    "$$P(C_2|x) = \\frac{P(x|C_2)P(C_2)}{P(x|C_1)P(C_1) + P(x|C_2)P(C_2)}$$\n",
    "\n",
    "Note that $P(C_1|x) + P(C_2|x) = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier\n",
    "\n",
    "Therefore, for a given test point $\\mathbf{x}^*$, our decision rule is:\n",
    "\n",
    "$$P(C_1|\\mathbf{x}^*) \\underset{C_2}{\\overset{C_1}{\\gtrless}} P(C_2|\\mathbf{x}^*)$$\n",
    "\n",
    "Using the Bayes' Theorem, we can further rewrite it as:\n",
    "\\begin{align}\n",
    "\\frac{P(\\mathbf{x}^*|C_1)P(C_1)}{P(\\mathbf{x}^*)} &\\underset{C_2}{\\overset{C_1}{\\gtrless}} \\frac{P(\\mathbf{x}^*|C_2)P(C_2)}{P(\\mathbf{x}^*)}\n",
    "\\end{align}\n",
    "This defines the **Naive Bayes Classifier**.\n",
    "\n",
    "* If the data likelihoods $p(\\mathbf{x}^*|C_1)$ and $p(\\mathbf{x}^*|C_2)$ are Gaussian-distributed, then we are utilizing the **Gaussian Naive Bayes Classifier**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Generative Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* So, **to train the classifier**, what we need to do is to determine the parametric forms and the associated parameters for $p(x|C_1)$, $p(x|C_2)$, $P(C_1)$ and $p(C_2)$.\n",
    "\n",
    "For example, we can assume that the data samples coming from either $C_1$ and $C_2$ are distributed according to Gaussian distributions. In this case, \n",
    "\n",
    "$$p(x|C_k) = \\frac{1}{(2\\pi)^{1/2} |\\Sigma_k|^{1/2}}\\exp\\left\\{-\\frac{1}{2}(\\mathbf{x}-\\mathbf{\\mu}_k)^T\\Sigma_k^{-1}(\\mathbf{x}-\\mathbf{\\mu}_k)\\right\\}, \\forall k=\\{1,2\\}$$\n",
    "\n",
    "* We can consider any distributional form we want.\n",
    "\n",
    "What about the $P(C_1)$ and $P(C_2)$?\n",
    "\n",
    "* We can consider the relative frequency of each class, that is, $P(C_i) = \\frac{N_i}{N}$, where $N_i$ is the number of points in class $C_i$ and $N$ is the total number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **What are the parameters that we need to estimate in order to train this classifier?**\n",
    "\n",
    "* **How would you estimate those parameters?**\n",
    "\n",
    "To be continued..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
