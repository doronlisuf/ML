{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 40 - Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection & Feature Extraction\n",
    "\n",
    "Note that feature selection and feature extraction are two different approaches.\n",
    "\n",
    "1. **Feature Selection:** Choosing a subset of the original pool of features.\n",
    "    \n",
    "2. **Feature Extraction:** Creating useful features from data.\n",
    "    * Dimensionality reduction techniques such as PCA are performing feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **feature selection**, we can have three main approaches:\n",
    "\n",
    "1. **Wrappers:** a wrapper evaluates a specific model sequentially using different potential subsets of features to get the subset that best works in the end. They are highly costly and have a high chance of overfitting, but also a high chance of success, on the other hand.\n",
    "    * Example: Forward Sequential Feature Selection, Backward Feature Elimination, etc.\n",
    "\n",
    "2. **Filters:** for a much faster alternative, filters do not test any particular algorithm, but rank the original features according to their relationship with the problem (labels) and just select the top of them. Correlation and mutual information are the most widespread criteria. There are many easy to use tools, like the [feature selection ```scikit-learn``` package](https://scikit-learn.org/stable/modules/feature_selection.html).\n",
    "\n",
    "    * Example: Correlation, chi-squared test, ANOVA, information gain, etc.\n",
    "\n",
    "3. **Embedded:** this group is made up of all the Machine Learning techniques that include feature selection during their training stage. \n",
    "    * Example: LASSO, Elastic Net, Ridge, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "\n",
    "A very common approach (and one of the simplest approaches) to **dimensionality reduction** is **Principal Component Analysis** (or **PCA**). \n",
    "\n",
    "* PCA takes data from *sensor coordinates* to *data centric coordinates* using linear transformations.\n",
    "\n",
    "PCA uses a **linear transformation** to **minimize the redundancy** of the resulting transformed data (by ending up with data that is uncorrelated).\n",
    "* This means that every transformed dimension is more informative.\n",
    "* In this approach, the dimensionality of the space is still the same as the original data, but the space of features are now arranged such that they contain the most information.\n",
    "\n",
    "If we wish to reduce dimensionality of our feature space, we can choose only the features that carry over the most information in the linearly transformed space.\n",
    "* In other words, PCA will find the underlying **linear manifold** that the data is embedded in.\n",
    "\n",
    "**PCA finds the directions of maximum variance in high-dimensional data and projects it onto a new subspace with equal or fewer dimensions than the original one**. \n",
    "\n",
    "There are a couple of points-of-view on how to find $A$: \n",
    "\n",
    "1. Maximum Variance Formulation\n",
    "2. Minimum-error Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PCA as Maximum Variance Formulation\n",
    "\n",
    "Consider the data $X$ comprised on $N$ data samples in a D-dimensional space, so $X$ is a matrix of size $D\\times N$.\n",
    "\n",
    "The **first step** in PCA is to centralize or demean $X$. This will is to guarantee that all features will have the same impact and not weight more only because their range of values is much larger (example, age vs income).\n",
    "\n",
    "* Without loss of generality, let's assume that we subtracted the mean to the input data, $X$. Now, $X$ has zero mean.\n",
    "\n",
    "The **second step** is to find the linear transformation $A$ that transforms $X$ to a space where features are:\n",
    "1. Uncorrelated (preserve all dimensions)\n",
    "2. reduced (dimensionality reduction)\n",
    "\n",
    "\\begin{align*}Y = AX\\end{align*}\n",
    "\n",
    "where $A$ is a $D\\times D$ matrix, $X$ is a $D\\times N$ data matrix and therefore $Y$ is also a $D\\times N$ transformed data matrix.\n",
    "\n",
    "The variance of the transformed data $Y$ is given by:\n",
    "\n",
    "\\begin{align*}\n",
    "R_y &= E[YY^T] \\\\\n",
    "&= E[AX(AX)^T] \\\\\n",
    "&= E[AXX^TA^T] \\\\\n",
    "&= AE[XX^T]A^T \\\\\n",
    "&= AR_xA^T\n",
    "\\end{align*}\n",
    "\n",
    "Note that we are computing the variance along the dimensions of $Y$, therefore, $R_y$ is a $D\\times D$ matrix. Similarly, $R_X$ represents the covariance of the data $X$. Covariances matrices are symmetric therefore $R_X=R_X^T$ and $R_Y=R_Y^T$.\n",
    "\n",
    "Similarly, $R_X$ represents the covariance of the data $X$. \n",
    "\n",
    "If we write $A$ in terms of vector elements:\n",
    "\n",
    "\\begin{align*}A = \\left[\\begin{array}{cc}\n",
    "a_{11} & a_{12}\\\\\n",
    "a_{21} & a_{22}\n",
    "\\end{array}\\right]=\\left[\\begin{array}{c}\n",
    "\\overrightarrow{a_{1}}\\\\\n",
    "\\overrightarrow{a_{2}}\n",
    "\\end{array}\\right]\\end{align*}\n",
    "\n",
    "Then,\n",
    "\n",
    "\\begin{align*}\n",
    "R_y &= \\left[\\begin{array}{c}a_{1}\\\\a_{2}\\end{array}\\right] R_X \\left[\\begin{array}{cc}a_{1} & a_{2}\\end{array}\\right] \\\\\n",
    "&= \\left[\\begin{array}{cc} a_1 R_X a_1^T & a_1 R_X a_2^T\\\\ a_2 R_X a_1^T & a_2 R_X a_2^T\\end{array}\\right]\n",
    "\\end{align*}\n",
    "\n",
    "* If we want to represent the data in a space in which the features are **uncorrelated**, what shape does the covariance matrix have to take?\n",
    "\n",
    "Diagonal! Why?\n",
    "\n",
    "* Can we use the eigenvectors of $R_X$ as our linear transformation $A$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the case where we are trying to project the data $X$ into a 1-dimensional space, so we are trying to find the direction $a_1$ where maximal data variance is preserved:\n",
    "\n",
    "\\begin{align*}\n",
    "\\arg_{a_1} \\max a_1 R_X a_1^T\\end{align*}\n",
    "\n",
    "We want this solution to be bounded (considering $a_1 = \\infty$ would maximize), so we need to constraint the vector to have norm 1\n",
    "\n",
    "\\begin{align*}\\Vert a_1\\Vert_2^2 = 1 \\iff a_1 a_1^T = 1\\end{align*}\n",
    "\n",
    "Then, we using Lagrange Optimization:\n",
    "\n",
    "\\begin{align*}\\mathcal{L} = a_1 R_X a_1^T + \\lambda_1 (1-a_1 a_1^T)\\end{align*}\n",
    "\n",
    "Solving for $a_1$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial a_1} &= 0 \\\\\n",
    "R_X a_1^T + R_X^T a_1^T - 2\\lambda_1 a_1^T &= 0 \\\\\n",
    "R_X a_1^T = \\lambda a_1^T\n",
    "\\end{align*}\n",
    "\n",
    "* Does this look familiar?\n",
    "\n",
    "This is stating that $a_1^T$ must be an eigenvector of $R_X$!\n",
    "\n",
    "So coming back to the question \"Can we use the eigenvectors of $X$ as our linear transformation $A$?\" YES!\n",
    "\n",
    "* If we left multiply by $a_1$ and make use of $a_1a_1^T = 1$:\n",
    "\n",
    "\\begin{align*}a_1 R_X a_1^T = \\lambda_1\\end{align*}\n",
    "\n",
    "**So the variance will be maximum when we set the project direction $a_1$ equal to the eigenvector having the largest eigenvalue $\\lambda_1$**.\n",
    "\n",
    "* This eigenvector is known as the firt **principal component**.\n",
    "\n",
    "* As you may anticipate, the linear trasnformation $A$ will the be a matrix whose row entries are **sorted** the eigenvectors (sorted by their correspondent eigenvalue).\n",
    "\n",
    "\\begin{align*}A &= \\left[\\begin{array}{c}\n",
    "\\overrightarrow{a_{1}}\\\\\n",
    "\\overrightarrow{a_{2}}\n",
    "\\end{array}\\right]\\end{align*}\n",
    "\n",
    "where $a_1$ and $a_2$ are eigenvectors of $R_X$ with correspondent eigenvalues $\\lambda_1$ and $\\lambda_2$ with $\\lambda_1>\\lambda_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PCA as Minimum-error Formulation\n",
    "\n",
    "We can also look at PCA as a minimization of mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "image/png": {
       "width": 300
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image('figures/Figure12.2.png',width=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider $X$ and a D-dimensional orthogonal basis $\\mathbf{u}$:\n",
    "\n",
    "\\begin{align*}\\hat{x} = \\sum_{i=1}^m y_i a_i\\end{align*}\n",
    "\n",
    "where $m<D$.\n",
    "\n",
    "\\begin{align*}y_j = x^Ta_j \\end{align*}\n",
    "\n",
    "where $A^TA = I$.\n",
    "\n",
    "We want to minimize the residual error:\n",
    "\n",
    "\\begin{align*}\\epsilon = x - \\hat{x} = \\sum_{i=m+1}^D y_i a_i\\end{align*}\n",
    "\n",
    "* The objective function we will use is the mean square residual:\n",
    "\n",
    "\\begin{align*}\n",
    "J &= E\\left[ \\|\\epsilon\\|^2_2\\right]\\\\\n",
    "&= E\\left[\\left( \\sum_{i=m+1}^D y_ia_i\\right)\\left( \\sum_{i=m+1}^D y_i a_i\\right) \\right]\\\\\n",
    "&=\\sum_{j=m+1}^D E [y_j^2]\\\\\n",
    "&=\\sum_{j=m+1}^D E [(a_j^T\\mathbf{x})(\\mathbf{x}^Ta_j)]\\\\\n",
    "&= \\sum_{j=m+1}^D a_j^T E[\\mathbf{x}\\mathbf{x}^T]a_j\\\\\n",
    "&= \\sum_{j=m+1}^D a_j^T R_x a_j\n",
    "\\end{align*}\n",
    "\n",
    "Minimize the error and incorporate Lagrange parameters for $U^TU=I$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J}{\\partial a_j} &= 2(R_x a_j - \\lambda_j a_j) = 0\\\\\n",
    "R_x a_j &= \\lambda_j a_j\n",
    "\\end{align*}\n",
    "\n",
    "So, the sum of the error is the sum of the eigenvalues of the unused eigenvectors.  So, we want to select the eigenvectors with the $m$ largest eigenvalues. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps of PCA\n",
    "\n",
    "Consider the data $X$:\n",
    "\n",
    "1. Subtract the mean $\\mu = \\frac{1}{N}\\sum_{i=1}^N x_i$\n",
    "\n",
    "2. Compute the covariance matrix $R_X$ (by definition, the covariance already subtracts the data's mean)\n",
    "\n",
    "3. Compute eigenvectors and eigenvalues of the matrix $R_X$, and store the sorted eigenvectors ($e_i$) in decreasing eigenvalue ($\\lambda_i$) order.\n",
    "\n",
    "4. The linear transformation $A$ will be: $A = \\left[\\begin{array}{c} \\mathbf{a_{1}}\\\\ \\mathbf{a_{2}} \\end{array}\\right]$, where $\\lambda_1 > \\lambda_2$\n",
    "\n",
    "5. Projection: $y=Ax$\n",
    "\n",
    "Note that the formal definition of covariance already accounts for demeaning the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA(X, m, display=1):\n",
    "    '''This function implements PCA. The data matrix X is DxN matrix, \n",
    "    where D is the dimension and N the number of points'''\n",
    "    \n",
    "    D, N = X.shape\n",
    "    \n",
    "    # Demean the Data\n",
    "    data = X - X.mean(axis=1).reshape(-1, 1)\n",
    "    \n",
    "    # Covariance of the input data X\n",
    "    cov_mat = np.cov(data)\n",
    "    \n",
    "    # Find eigenvectors and eigenvalues \n",
    "    eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)\n",
    "    \n",
    "    # Sort eigenvectors by magnitude of eigenvalues\n",
    "    eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:,i]) for i in range(len(eigen_vals))]\n",
    "    eigen_pairs = sorted(eigen_pairs, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    # Linear transformation\n",
    "    A = np.hstack(([eigen_pairs[i][1][:, np.newaxis] for i in range(m)]))\n",
    "    \n",
    "    #compute explained variance and plot\n",
    "    cum_var_exp=0\n",
    "    if display:\n",
    "        total = sum(eigen_vals)\n",
    "        var_explained = [(i/total) for i in sorted(eigen_vals, reverse=True)]\n",
    "        cum_var_exp = np.cumsum(var_explained)\n",
    "        plt.bar(range(1,D+1), var_explained, alpha=0.5, align='center', label='individual explained variance')\n",
    "        plt.step(range(1,D+1), cum_var_exp, alpha=0.5, where='mid', label='cumulative explained variance')\n",
    "        plt.ylabel('Explained variance ratio')\n",
    "        plt.xlabel('Principal components')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show();\n",
    "    return A, eigen_pairs, cum_var_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Half-Moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples = 100, random_state = 123)\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c=y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO BE FINISHED IN CLASS\n",
    "\n",
    "plt.scatter(projection_PCA, np.zeros(len(projection_PCA)), c=y)\n",
    "plt.title('PCA Projection', fontsize=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "model = LDA(n_components=1)\n",
    "projection_LDA = model.fit_transform(X,y)\n",
    "\n",
    "plt.scatter(projection_LDA, np.zeros(len(projection_LDA)), c=y)\n",
    "plt.title('LDA Projection', fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Synthetic oval-shaped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "X, y = datasets.make_blobs(n_samples=1500,centers=2,cluster_std=2.5) \n",
    "X    = np.dot(X, [[0.60834549, -0.63667341], [-0.40887718, 0.85253229]])\n",
    "plt.scatter(X[y==0,0],X[y==0,1],c='r')\n",
    "plt.scatter(X[y==1,0],X[y==1,1],c='b');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A,_,_ = PCA(X.T,1,0)\n",
    "projection_PCA = X@A\n",
    "\n",
    "model = LDA(n_components=1)\n",
    "projection_LDA = model.fit_transform(X,y)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.scatter(projection_PCA[y==0], np.zeros(len(projection_PCA[y==0])), c='r')\n",
    "plt.scatter(projection_PCA[y==1], 0.01+np.zeros(len(projection_PCA[y==1])), c='b')\n",
    "plt.ylim([-0.1,0.1]);plt.title('PCA Projection', fontsize=15);\n",
    "plt.subplot(122)\n",
    "plt.scatter(projection_LDA[y==0], np.zeros(len(projection_LDA[y==0])), c='r')\n",
    "plt.scatter(projection_LDA[y==1], 0.01+np.zeros(len(projection_LDA[y==1])), c='b')\n",
    "plt.ylim([-0.1,0.1]); plt.title('LDA Projection', fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Wine Dataset\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "\n",
    "wine_data = load_wine(return_X_y=False)\n",
    "# print(wine_data.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = wine_data.data\n",
    "y = wine_data.target\n",
    "\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, _, cum_var_exp= PCA(X.T,2,1)\n",
    "\n",
    "projection = X@A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_var_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(cum_var_exp>=0.9)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "plt.scatter(projection[:, 0], projection[:, 1], c=y)\n",
    "plt.xlabel('Principal Component 1', fontsize=15)\n",
    "plt.ylabel('Principal Component 2', fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: MNIST Dataset Handwritten Digits\n",
    "\n",
    "(http://yann.lecun.com/exdb/mnist/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(images, labels), (images_test, labels_test) = mnist.load_data()\n",
    "\n",
    "images = images/255.\n",
    "images_test = images_test/255.\n",
    "print(images.shape,labels.shape,images_test.shape,labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "for i in range(15):\n",
    "    idx = npr.choice(len(images),size=1)\n",
    "    plt.subplot(3,5,i+1)\n",
    "    plt.imshow(np.squeeze(images[idx,:,:],axis=0), cmap='gray')\n",
    "    plt.title(labels[idx][0])\n",
    "    plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flattening image into vector\n",
    "N,D,_ = images.shape\n",
    "Ntest,D,_ = images_test.shape\n",
    "\n",
    "X = images.flatten().reshape(N, D*D)\n",
    "X_test = images_test.flatten().reshape(Ntest, D*D)\n",
    "\n",
    "X.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, eigen_pairs, cum_var_exp = PCA(X.T,2,0)\n",
    "\n",
    "y = X@A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.suptitle('MNIST Top 9 Eigenvectors', size=16)\n",
    "for i in range(9):\n",
    "        plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(np.real(eigen_pairs[i][1].reshape(28,28)),cmap='gray')\n",
    "        plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "plt.scatter(np.real(y[:, 0]), np.real(y[:, 1]), c=labels, edgecolor='none', alpha=0.8, cmap=plt.cm.get_cmap('jet', 10))\n",
    "plt.colorbar(ticks=range(10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "PCA is an **unsupervised** model that can be used to:\n",
    "\n",
    "1. Perform Dimensionality Reduction, by projecting data into directions with maximum explained variance\n",
    "2. Uncorrelate Data, by rotating the data space such that data becomes uncorrelated\n",
    "\n",
    "PCA uses **linear transformations** (projections or rotations) of the input data $X$:\n",
    "\n",
    "$$Y=AX$$\n",
    "\n",
    "where $A$ is a $D\\times D$ linear transformation matrix, $X$ is an $D\\times N$ data matrix, and $Y$ is a $D\\times N$ transformed data matrix.\n",
    "\n",
    "Therefore PCA will work well when the relationship between features are linear. Moreover, PCA is unsupervised because it **does not** use the class labels to find vector projections (or rotations). So the projections may not necessarily be in the direction that maximize class separability.\n",
    "\n",
    "PCA can be formulated from two points-of-view:\n",
    "1. Maximum explained variance\n",
    "2. Minimum reconstruction error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel PCA\n",
    "\n",
    "There are other variants of PCA such as **Kernel PCA**, where we first project the data to a space where classes are linearly separable (RBF kernel) and then apply PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "X, y = make_circles(n_samples=1500, noise=0.1, factor=0.2)\n",
    "plt.scatter(X[:,0],X[:,1],c=y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "\n",
    "model_pca = PCA(n_components=1)\n",
    "proj = model_pca.fit_transform(X)\n",
    "\n",
    "model_kpca = KernelPCA(n_components=1,kernel='rbf',gamma=2)\n",
    "proj_kpca = model_kpca.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121); plt.scatter(proj,np.ones(len(proj)),c=y)\n",
    "plt.title('PCA Projection', size=15)\n",
    "plt.subplot(122); plt.scatter(proj_kpca,np.ones(len(proj_kpca)),c=y)\n",
    "plt.title('Kernel PCA Projection with RBF kernel',size=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manifold Learning\n",
    "\n",
    "As we have already noted, many natural sources of data correspond to low-dimensional, possibly noisy, non-linear manifolds embedded within the higher dimensional observed data space. Capturing this property explicitly can lead to improved density modelling compared with more general methods.\n",
    "\n",
    "PCA and LDA are often used to project a data set onto a lower-dimensional space. However both of then assume that the data samples *live* in an underlying linear manifold.\n",
    "\n",
    "There are other dimensionality reduction techniques that do not assume the manifold is linear. They include:\n",
    "\n",
    "1. **Multi-Dimensional Scaling (MDS)**\n",
    "\n",
    "2. **Isometric Mapping (ISOMAP)**\n",
    "\n",
    "3. **Locally Linear Embedding (LLE)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiDimensional Scaling (MDS)\n",
    "\n",
    "Another linear technique with a similar aim is **multidimensional scaling**, or **MDS**. It finds a low-dimensional projection of the data such as to preserve, as closely as possible, the pairwise distances between data points, and involves finding the eigenvectors of the distance matrix. In the case where the distances are Euclidean, it gives equivalent results to PCA. Therefore, MDS is a generalization of PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a set of mean-centered observations $X=\\{x_1,x_2,\\dots,x_N\\}$ where $x_i\\in\\mathbb{R}^D$. By mean-centered samples $X$, I mean that $\\mu_j = \\sum_{i=1}^N x_{ij} = 0, \\forall j=1,2,\\dots,D$.\n",
    "\n",
    "Consider the **proximity matrix** $D$ that stores pairwise distances of data points $d_{ij} = \\text{distance}(x_i,x_j)$:\n",
    "\n",
    "$$D = \\left[\\begin{array}{cccc}\n",
    "d_{11} & d_{12} & \\cdots & d_{1N}\\\\\n",
    "d_{21} & d_{22} & \\cdots & d_{2N}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "d_{N1} & d_{N2} & \\cdots & d_{NN}\n",
    "\\end{array}\\right]$$\n",
    "\n",
    "Note that $D$ is an $N\\times N$ symmetric matrix.\n",
    "\n",
    "A proximity matrix is:\n",
    "* A *metric* if \n",
    "    1. $d_{ii}=0$\n",
    "    2. $d_{ij}\\geq 0, i\\neq j$\n",
    "    3. $d_{ij} = d_{ji},\\forall i,j$\n",
    "    4. $d_{ij} \\leq d_{ik} + d_{jk},\\forall i,j,k$ (triangle inequality).\n",
    "* *Euclidean* if there exists a configuration of points in Euclidean space with the same $d_{ij}$ values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an assumed Euclidean proximity matrix, $D$, the **goal** of MDS is to find a set of points, $Y$, that have the same proximity matrix in an M-dimensional space, where $M<D$.\n",
    "\n",
    "Let:\n",
    "\n",
    "$$B = YY^T = \\left[\\begin{array}{c} y_{1}\\\\ y_{2}\\\\ \\vdots\\\\ y_{N} \\end{array}\\right]\\left[\\begin{array}{cccc}\n",
    "y_{1}^{T} & y_{2}^{T} & \\cdots & y_{N}^{T}\\end{array}\\right]=\\left[\\begin{array}{cccc}\n",
    "y_{1}y_{1}^{T} & y_{1}y_{2}^{T} & \\cdots & y_{1}y_{N}^{T}\\\\\n",
    "y_{2}y_{1}^{T} & y_{2}y_{2}^{T} & \\cdots & y_{2}y_{N}^{T}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "y_{N}y_{1}^{T} & y_{N}y_{2}^{T} & \\cdots & y_{N}y_{N}^{T}\n",
    "\\end{array}\\right]$$\n",
    "\n",
    "Then $$b_{ij} = \\sum_{k=1}^M y_{ik}y_{jk}$$\n",
    "\n",
    "* So, if we want to find $B$, then we can determine $Y$ by taking: \n",
    "\n",
    "$$Y\\approx B^{1/2}$$\n",
    "\n",
    "since $B=YY^T$.\n",
    "\n",
    "* The squared Euclidean distance between points of M-dimensional data $Y$ can be written in terms of $B$:\n",
    "\n",
    "\\begin{align*}\n",
    "d_{ij}^2 &= (y_i - y_j) (y_i - y_j)^T \\\\\n",
    "&= (y_i - y_j) (y_i^T - y_j^T) \\\\\n",
    "&= y_iy_i^T - y_iy_j^T -y_jy_i^T + y_jy_j^T \\\\\n",
    "&= y_iy_i^T - 2 y_iy_j^T + y_jy_j^T \\\\\n",
    "&= b_{ii} - 2 b_{ij} + b_{jj} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "* Note that if we *translate* or *rotate* the data, we get the same proximity matrix! \n",
    "\n",
    "* Let's add some constraints to our transformed data: constraint data $Y$ to have mean zero for all dimensions, which is to say, $\\sum_{i=1}^N y_{ik} = 0, \\forall k$.\n",
    "\n",
    "* This implies that each row and column of $B$ sum to 0. Proof:\n",
    "\n",
    "$$\\sum_{j=1}^N b_{ij} = \\sum_{j=1}^N\\sum_{k=1}^M y_{ik}y_{jk} = \\sum_{k=1}^M y_{ik} \\left(\\sum_{j=1}^N y_{jk}\\right) = 0 \\text{ (sum of columns)} $$\n",
    "\n",
    "$$\\sum_{i=1}^N b_{ij} = \\sum_{i=1}^N\\sum_{k=1}^M y_{jk}y_{ik} = \\sum_{k=1}^M y_{ik} \\left(\\sum_{j=1}^N y_{jk}\\right) = 0 \\text{ (sum of rows)} $$\n",
    "\n",
    "Given this, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{i=1}^N d_{ij}^2 &= \\sum_{i=1}^N (b_{ii} + b_{jj} - 2b_{ij}) = \\sum_{i=1}^N b_{ii} + \\sum_{i=1}^N b_{jj} - \\sum_{i=1}^N 2b_{ij} = T + Nb_{jj}\\\\\n",
    "\\iff b_{jj} &= \\frac{1}{N}\\left(\\sum_{i=1}^N d_{ij}^2 - T\\right) = \\frac{1}{N}\\left(\\sum_{i=1}^N d_{ij}^2 - \\frac{1}{2N}\\sum_{i=1}^N\\sum_{j=1}^N d_{ij}^2\\right)\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{j=1}^N d_{ij}^2 &= \\sum_{j=1}^N (b_{ii} + b_{jj} - 2b_{ij}) = \\sum_{j=1}^N b_{ii} + \\sum_{j=1}^N b_{jj} - \\sum_{j=1}^N 2b_{ij} = T + Nb_{ii}\\\\\n",
    "\\iff b_{ii} &= \\frac{1}{N}\\left(\\sum_{j=1}^N d_{ij}^2 - T\\right) = \\frac{1}{N}\\left(\\sum_{j=1}^N d_{ij}^2 - \\frac{1}{2N}\\sum_{i=1}^N\\sum_{j=1}^N d_{ij}^2\\right)\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{i=1}^N\\sum_{j=1}^N d_{ij}^2 &= \\sum_{i=1}^N\\sum_{j=1}^N (b_{ii} + b_{jj} - 2b_{ij}) = 2NT \\\\\n",
    "\\iff T &= \\frac{1}{2N} \\sum_{i=1}^N\\sum_{j=1}^N d_{ij}^2\n",
    "\\end{align*}\n",
    "\n",
    "where $T = \\text{trace}(B) = \\sum_{i=1}^N b_{ii}$.\n",
    "\n",
    "\n",
    "* Coming back to proximity matrix, $D$:\n",
    "\\begin{align*}\n",
    "d_{ij}^2 &= b_{ii} - 2 b_{ij} + b_{jj} \\\\\n",
    "d_{ij}^2 &= \\frac{1}{N}\\left(\\sum_{j=1}^N d_{ij}^2 - \\frac{1}{2N}\\sum_{i=1}^N\\sum_{j=1}^N d_{ij}^2\\right) - 2b_{ij} + \\frac{1}{N}\\left(\\sum_{i=1}^N d_{ij}^2 - \\frac{1}{2N}\\sum_{i=1}^N\\sum_{j=1}^N d_{ij}^2\\right) \\\\\n",
    "2b_{ij} &= -d_{ij}^2 + \\frac{1}{N}\\sum_{j=1}^N d_{ij}^2 + \\frac{1}{N}\\sum_{i=1}^N d_{ij}^2 - \\frac{1}{N^2}\\sum_{i=1}^N\\sum_{j=1}^N d_{ij}^2 \\\\\n",
    "b_{ij} &= -\\frac{1}{2} \\left(d_{ij}^2 - \\frac{1}{N}\\sum_{j=1}^N d_{ij}^2 - \\frac{1}{N}\\sum_{i=1}^N d_{ij}^2 + \\frac{1}{N^2}\\sum_{i=1}^N\\sum_{j=1}^N d_{ij}^2\\right)\n",
    "\\end{align*}\n",
    "\n",
    "* So, now, we can estimate $B$ using the proximity matrix $D$. In a matrix form, we can write:\n",
    "\n",
    "$$B = -\\frac{1}{2} JD^{2}J$$\n",
    "\n",
    "where $J = I - \\frac{1}{N}\\mathbf{1}\\mathbf{1}^T$, $I$ is an $N\\times N$ identity matrix, $\\mathbf{1}$ is an $N\\times 1$ vector of 1's and $D^{2} = \\left[d_{ij}^2\\right]$ is the proximity matrix of size $N\\times N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Recall that for any real symmetruc matrix $B$, the eigenvalues are real and the eigenvectors can be chosen such that they are orthogonal to each other. Thus a real symmetric matrix $B$ can be described in the eigenspace, as follows:\n",
    "\n",
    "$$B=V\\Lambda V^T$$\n",
    "\n",
    "where $V$ is an orthogonal matrix containing the eigenvectors of $B$ and $\\Lambda$ is a diagonal matrix containing the eigenvalues of $B$.\n",
    "\n",
    "Then we can estimate $Y$ as follows:\n",
    "\n",
    "$$Y = B^{1/2} = V\\Lambda^{1/2}$$\n",
    "\n",
    "keeping only the $M$ dimensions of interest ($M<D$) corresponding to the $M$ largest eigenvalues.\n",
    "\n",
    "* Note: If we use the Euclidean distance to compute $D$ then MDS is equivalent to PCA! However, $D$ can be computed with any metric and therefore, MDS is a generalization of PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to Implement MDS\n",
    "\n",
    "The MDS algorithm:\n",
    "\n",
    "1. Compute the distance/proximity matrix, $D$.\n",
    "2. Compute $D^2$.\n",
    "3. Compute $J = I - \\frac{1}{N}\\mathbf{1}\\mathbf{1}^T$\n",
    "4. Compute $B = -\\frac{1}{2} JD^{2}J$\n",
    "5. Compute eigenvectors, $V$, and eigenvalues,$\\Lambda$, of $B$. (store them in a matrix in decreasing eigenvalue order.)\n",
    "6. Compute $Y = V\\Lambda^{1/2}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
