{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 41 - Manifold Learning: MDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review of PCA\n",
    "\n",
    "PCA is an **unsupervised** model that can be used to:\n",
    "\n",
    "1. Perform Dimensionality Reduction, by projecting data into directions with maximum explained variance\n",
    "2. Uncorrelate Data, by rotating the data space such that data becomes uncorrelated\n",
    "\n",
    "PCA uses **linear transformations** (projections or rotations) of the input data $X$:\n",
    "\n",
    "$$Y=AX$$\n",
    "\n",
    "where $A$ is a $D\\times D$ linear transformation matrix, $X$ is an $D\\times N$ data matrix, and $Y$ is a $D\\times N$ transformed data matrix.\n",
    "\n",
    "Therefore PCA will work well when the relationship between features are linear. Moreover, PCA is unsupervised because it **does not** use the class labels to find vector projections (or rotations). So the projections may not necessarily be in the direction that maximize class separability.\n",
    "\n",
    "PCA can be formulated from two points-of-view:\n",
    "1. Maximum explained variance\n",
    "2. Minimum reconstruction error\n",
    "\n",
    "## Steps of PCA\n",
    "\n",
    "Consider the data $X$:\n",
    "\n",
    "1. Subtract the mean $\\mu = \\frac{1}{N}\\sum_{i=1}^N x_i$\n",
    "\n",
    "2. Compute the covariance matrix $R_X$ (by definition, the covariance already subtracts the data's mean)\n",
    "\n",
    "3. Compute eigenvectors and eigenvalues of the matrix $R_X$, and store the sorted eigenvectors ($e_i$) in decreasing eigenvalue ($\\lambda_i$) order.\n",
    "\n",
    "4. The linear transformation $A$ will be: $A = \\left[\\begin{array}{c} \\mathbf{a_{1}}\\\\ \\mathbf{a_{2}} \\end{array}\\right]$, where $\\lambda_1 > \\lambda_2$\n",
    "\n",
    "5. Projection: $\\mathbf{Y}=A\\mathbf{X}$\n",
    "\n",
    "Note that the formal definition of covariance already accounts for demeaning the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Consider the scaled data matrix $X$ of size $D\\times N$ (where $N$ is the number of samples and $D$ the number of features), with covariance matrix $K$ of size $D\\times D$. The eigenvalues of $K$ are $\\lambda_1=0.99$, $\\lambda_2=0.5$ and $\\lambda_3=2$, and the respective eigenvectors are $v_1=\\left[\\begin{array}{c}-0.99, 0.09,0\\end{array}\\right]^T$, $v_2=\\left[\\begin{array}{c}0,0,1\\end{array}\\right]^T$ and $v_3=\\left[\\begin{array}{c}0.09,0.99,0\\end{array}\\right]^T$. Answer the following questions:\n",
    "\n",
    "1. Suppose $Y$ is the data transformation obtained by principal component transform of $X$ into a 2-dimensional space. Using the information provided, write down the linear transformation matrix $A$, and write down the formula for computing $Y$ from $X$ and $A$. Justify your answer.\n",
    "2. What is the amount of explained variance of the 2-D projection? Justify your answer.\n",
    "3. What is the covariance matrix of $Y$? Justify your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8567335243553009"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(2+0.99)/(2+0.99+0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manifold Learning\n",
    "\n",
    "As we have already noted, many natural sources of data correspond to low-dimensional, possibly noisy, non-linear manifolds embedded within the higher dimensional observed data space. Capturing this property explicitly can lead to improved density modelling compared with more general methods.\n",
    "\n",
    "PCA and LDA are often used to project a data set onto a lower-dimensional space. However both of then assume that the data samples *live* in an underlying linear manifold.\n",
    "\n",
    "There are other dimensionality reduction techniques that do not assume the manifold is linear. They include:\n",
    "\n",
    "1. **Multi-Dimensional Scaling (MDS)**\n",
    "\n",
    "2. **Isometric Mapping (ISOMAP)**\n",
    "\n",
    "3. **Locally Linear Embedding (LLE)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiDimensional Scaling (MDS)\n",
    "\n",
    "Another linear technique with a similar aim is **multidimensional scaling**, or **MDS**. It finds a low-dimensional projection of the data such as to preserve, as closely as possible, the pairwise distances between data points, and involves finding the eigenvectors of the distance matrix. In the case where the distances are Euclidean, it gives equivalent results to PCA. Therefore, MDS is a generalization of PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a set of mean-centered observations $X=\\{x_1,x_2,\\dots,x_N\\}$ where $x_i\\in\\mathbb{R}^D$. By mean-centered samples $X$, I mean that $\\mu_j = \\sum_{i=1}^N x_{ij} = 0, \\forall j=1,2,\\dots,D$.\n",
    "\n",
    "Consider the **proximity matrix** $D$ that stores pairwise distances of data points $d_{ij} = \\text{distance}(x_i,x_j)$:\n",
    "\n",
    "$$D = \\left[\\begin{array}{cccc}\n",
    "d_{11} & d_{12} & \\cdots & d_{1N}\\\\\n",
    "d_{21} & d_{22} & \\cdots & d_{2N}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "d_{N1} & d_{N2} & \\cdots & d_{NN}\n",
    "\\end{array}\\right]$$\n",
    "\n",
    "Note that $D$ is an $N\\times N$ symmetric matrix.\n",
    "\n",
    "A proximity matrix is:\n",
    "* A *metric* if \n",
    "    1. $d_{ii}=0$\n",
    "    2. $d_{ij}\\geq 0, i\\neq j$\n",
    "    3. $d_{ij} = d_{ji},\\forall i,j$\n",
    "    4. $d_{ij} \\leq d_{ik} + d_{jk},\\forall i,j,k$ (triangle inequality).\n",
    "* *Euclidean* if there exists a configuration of points in Euclidean space with the same $d_{ij}$ values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an assumed Euclidean proximity matrix, $D$, the **goal** of MDS is to find a set of points, $Y$, that have the same proximity matrix in an M-dimensional space, where $M<D$.\n",
    "\n",
    "Let:\n",
    "\n",
    "$$B = YY^T = \\left[\\begin{array}{c} y_{1}\\\\ y_{2}\\\\ \\vdots\\\\ y_{N} \\end{array}\\right]\\left[\\begin{array}{cccc}\n",
    "y_{1}^{T} & y_{2}^{T} & \\cdots & y_{N}^{T}\\end{array}\\right]=\\left[\\begin{array}{cccc}\n",
    "y_{1}y_{1}^{T} & y_{1}y_{2}^{T} & \\cdots & y_{1}y_{N}^{T}\\\\\n",
    "y_{2}y_{1}^{T} & y_{2}y_{2}^{T} & \\cdots & y_{2}y_{N}^{T}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "y_{N}y_{1}^{T} & y_{N}y_{2}^{T} & \\cdots & y_{N}y_{N}^{T}\n",
    "\\end{array}\\right]$$\n",
    "\n",
    "Then $$b_{ij} = \\sum_{k=1}^M y_{ik}y_{jk}$$\n",
    "\n",
    "* So, if we want to find $B$, then we can determine $Y$ by taking: \n",
    "\n",
    "$$Y\\approx B^{1/2}$$\n",
    "\n",
    "since $B=YY^T$.\n",
    "\n",
    "* The squared Euclidean distance between points of M-dimensional data $Y$ can be written in terms of $B$:\n",
    "\n",
    "\\begin{align*}\n",
    "d_{ij}^2 &= (y_i - y_j) (y_i - y_j)^T \\\\\n",
    "&= (y_i - y_j) (y_i^T - y_j^T) \\\\\n",
    "&= y_iy_i^T - y_iy_j^T -y_jy_i^T + y_jy_j^T \\\\\n",
    "&= y_iy_i^T - 2 y_iy_j^T + y_jy_j^T \\\\\n",
    "&= b_{ii} - 2 b_{ij} + b_{jj} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "* Note that if we *translate* or *rotate* the data, we get the same proximity matrix! \n",
    "\n",
    "* Let's add some constraints to our transformed data: constraint data $Y$ to have mean zero for all dimensions, which is to say, $\\sum_{i=1}^N y_{ik} = 0, \\forall k$.\n",
    "\n",
    "* This implies that each row and column of $B$ sum to 0. Proof:\n",
    "\n",
    "$$\\sum_{j=1}^N b_{ij} = \\sum_{j=1}^N\\sum_{k=1}^M y_{ik}y_{jk} = \\sum_{k=1}^M y_{ik} \\left(\\sum_{j=1}^N y_{jk}\\right) = 0 \\text{ (sum of columns)} $$\n",
    "\n",
    "$$\\sum_{i=1}^N b_{ij} = \\sum_{i=1}^N\\sum_{k=1}^M y_{jk}y_{ik} = \\sum_{k=1}^M y_{ik} \\left(\\sum_{j=1}^N y_{jk}\\right) = 0 \\text{ (sum of rows)} $$\n",
    "\n",
    "Given this, we have:\n",
    "\n",
    "Summing over the rows:\n",
    "\\begin{align*}\n",
    "\\sum_{i=1}^N d_{ij}^2 &= \\sum_{i=1}^N (b_{ii} + b_{jj} - 2b_{ij}) = \\sum_{i=1}^N b_{ii} + \\sum_{i=1}^N b_{jj} - \\sum_{i=1}^N 2b_{ij} = T + Nb_{jj}\\\\\n",
    "\\iff b_{jj} &= \\frac{1}{N}\\left(\\sum_{i=1}^N d_{ij}^2 - T\\right) = \\frac{1}{N}\\left(\\sum_{i=1}^N d_{ij}^2 - \\frac{1}{2N}\\sum_{i=1}^N\\sum_{j=1}^N d_{ij}^2\\right)\n",
    "\\end{align*}\n",
    "\n",
    "Summing over the columns:\n",
    "\\begin{align*}\n",
    "\\sum_{j=1}^N d_{ij}^2 &= \\sum_{j=1}^N (b_{ii} + b_{jj} - 2b_{ij}) = \\sum_{j=1}^N b_{ii} + \\sum_{j=1}^N b_{jj} - \\sum_{j=1}^N 2b_{ij} = T + Nb_{ii}\\\\\n",
    "\\iff b_{ii} &= \\frac{1}{N}\\left(\\sum_{j=1}^N d_{ij}^2 - T\\right) = \\frac{1}{N}\\left(\\sum_{j=1}^N d_{ij}^2 - \\frac{1}{2N}\\sum_{i=1}^N\\sum_{j=1}^N d_{ij}^2\\right)\n",
    "\\end{align*}\n",
    "\n",
    "Summing over the rows and columns:\n",
    "\\begin{align*}\n",
    "\\sum_{i=1}^N\\sum_{j=1}^N d_{ij}^2 &= \\sum_{i=1}^N\\sum_{j=1}^N (b_{ii} + b_{jj} - 2b_{ij}) = 2NT \\\\\n",
    "\\iff T &= \\frac{1}{2N} \\sum_{i=1}^N\\sum_{j=1}^N d_{ij}^2\n",
    "\\end{align*}\n",
    "\n",
    "where $T = \\text{trace}(B) = \\sum_{i=1}^N b_{ii}$.\n",
    "\n",
    "\n",
    "* Coming back to proximity matrix, $D$:\n",
    "\\begin{align*}\n",
    "d_{ij}^2 &= b_{ii} - 2 b_{ij} + b_{jj} \\\\\n",
    "d_{ij}^2 &= \\frac{1}{N}\\left(\\sum_{j=1}^N d_{ij}^2 - \\frac{1}{2N}\\sum_{i=1}^N\\sum_{j=1}^N d_{ij}^2\\right) - 2b_{ij} + \\frac{1}{N}\\left(\\sum_{i=1}^N d_{ij}^2 - \\frac{1}{2N}\\sum_{i=1}^N\\sum_{j=1}^N d_{ij}^2\\right) \\\\\n",
    "2b_{ij} &= -d_{ij}^2 + \\frac{1}{N}\\sum_{j=1}^N d_{ij}^2 + \\frac{1}{N}\\sum_{i=1}^N d_{ij}^2 - \\frac{1}{N^2}\\sum_{i=1}^N\\sum_{j=1}^N d_{ij}^2 \\\\\n",
    "b_{ij} &= -\\frac{1}{2} \\left(d_{ij}^2 - \\frac{1}{N}\\sum_{j=1}^N d_{ij}^2 - \\frac{1}{N}\\sum_{i=1}^N d_{ij}^2 + \\frac{1}{N^2}\\sum_{i=1}^N\\sum_{j=1}^N d_{ij}^2\\right)\n",
    "\\end{align*}\n",
    "\n",
    "* So, now, we can estimate $B$ using the proximity matrix $D$. In a matrix form, we can write:\n",
    "\n",
    "$$B = -\\frac{1}{2} JD^{2}J$$\n",
    "\n",
    "where $J = I - \\frac{1}{N}\\mathbf{1}\\mathbf{1}^T$, $I$ is an $N\\times N$ identity matrix, $\\mathbf{1}$ is an $N\\times 1$ vector of 1's and $D^{2} = \\left[d_{ij}^2\\right]$ is the proximity matrix of size $N\\times N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Recall that for any real symmetruc matrix $B$, the eigenvalues are real and the eigenvectors can be chosen such that they are orthogonal to each other. Thus a real symmetric matrix $B$ can be described in the eigenspace, as follows:\n",
    "\n",
    "$$B=V\\Lambda V^T$$\n",
    "\n",
    "where $V$ is an orthogonal matrix containing the eigenvectors of $B$ and $\\Lambda$ is a diagonal matrix containing the eigenvalues of $B$.\n",
    "\n",
    "Then we can estimate $Y$ as follows:\n",
    "\n",
    "$$Y = B^{1/2} = V\\Lambda^{1/2}$$\n",
    "\n",
    "keeping only the $M$ dimensions of interest ($M<D$) corresponding to the $M$ largest eigenvalues.\n",
    "\n",
    "* Note: If we use the Euclidean distance to compute $D$ then MDS is equivalent to PCA! However, $D$ can be computed with any metric and therefore, MDS is a generalization of PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to Implement MDS\n",
    "\n",
    "The MDS algorithm:\n",
    "\n",
    "1. Compute the distance/proximity matrix, $D$.\n",
    "2. Compute $D^2$.\n",
    "3. Compute $J = I - \\frac{1}{N}\\mathbf{1}\\mathbf{1}^T$\n",
    "4. Compute $B = -\\frac{1}{2} JD^{2}J$\n",
    "5. Compute eigenvectors, $V$, and eigenvalues,$\\Lambda$, of $B$. (store them in a matrix in decreasing eigenvalue order.)\n",
    "6. Compute $Y = V\\Lambda^{1/2}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
