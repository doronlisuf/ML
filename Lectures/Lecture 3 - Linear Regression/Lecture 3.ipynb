{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 3 - Linear Regression with Polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by considering the polynomial curve fitting example in the first chapter of our textbook. Polynomial (linear) regression is a supervised learning algorithm.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Polynomial Regression** is a type of liner regression that uses a special set of *features* - polynomial features.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Step 1 - Input Space</b> \n",
    "\n",
    "Suppose we are given a training set comprising of $N$ observations of $\\mathbf{x}$, $\\mathbf{x} = \\left[x_1, x_2, \\ldots, x_N \\right]^T$, and its corresponding desired outputs $\\mathbf{t} = \\left[t_1, t_2, \\ldots, t_N\\right]^T$, where sample $x_i$ has the desired label $t_i$.\n",
    "\n",
    "So, we want to learn the *true* function mapping $f$ such that $\\mathbf{t}  = f(\\mathbf{x}, \\mathbf{w})$, where $\\mathbf{w}$ are parameters of the model.\n",
    "</div>\n",
    "\n",
    "* We generally organize data into *vectors* and *matrices*. Not only is it a common way to organize the data, but it allows us to easily apply linear algebraic operations during analysis. It also makes it much simpler when it comes to code implementation!\n",
    "    * In engineering textbooks and for our course purposes, **vectors** are defined as *column vectors*. This is why we write $\\mathbf{x} = \\left[x_1, x_2, \\ldots, x_N \\right]^T$.\n",
    "\n",
    "* Note that both the training data and desired outputs can be noisy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>Step 2 - Feature Extraction</b> \n",
    "\n",
    "For the polynomial regression problem, let's consider *polynomial features* for each data point $x_i$. Let's say we can find these features using a **basis function**, $\\phi(\\mathbf{x})$. In the *polynomial regression* example, let's consider $\\phi(x_i) = \\left[x_{i}^{0}, x_{i}^{1}, x_{i}^{2}, \\dots, x_{i}^{M-1}\\right]^T$.\n",
    "</div>\n",
    "\n",
    "* Other features can be extracted.\n",
    "\n",
    "* For all data observations $\\{x_i\\}_{i=1}^N$ and using the feature space defined as $\\phi(x_i) = \\left[x_{i}^{0}, x_{i}^{1}, x_{i}^{2}, \\dots, x_{i}^{M-1}\\right]^T$, we can write the input data in a *matrix* form as:\n",
    "\n",
    "$$\\mathbf{X} =\\left[\\begin{array}{c} \\phi(x_1)^T \\\\ \\phi(x_2)^T \\\\ \\vdots \\\\ \\phi(x_N)^T \\end{array}\\right]  = \\left[\\begin{array}{ccccc}\n",
    "1 & x_{1} & x_{1}^{2} & \\cdots & x_{1}^{M}\\\\\n",
    "1 & x_{2} & x_{2}^{2} & \\cdots & x_{2}^{M}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "1 & x_{N} & x_{N}^{2} & \\cdots & x_{N}^{M}\n",
    "\\end{array}\\right] \\in \\mathbb{R}^{N\\times (M+1)}$$\n",
    "\n",
    "where each row is a feature representation of a data point $x_i$.\n",
    "\n",
    "Other **basis functions** include:\n",
    "\n",
    "* Polynomials Basis functions: $\\phi_j(x) = x^j$\n",
    "\n",
    "* Radial Basis functions: $\\phi_j(x) = \\exp\\left\\{-\\frac{(x-\\mu_j)^2)}{2s^2}\\right\\}$\n",
    "\n",
    "* Sigmoidal basis function: $\\phi_j(x) = \\sigma\\left(\\frac{x-\\mu_j}{s}\\right)$, where $\\sigma(a)$ is the logistic sigmoid function defined by $\\sigma(a)=\\frac{1}{1+\\exp(-a)}$\n",
    "\n",
    "* Fourier Basis functions, which leads to an expansion in sinusoidal functions. Each basis function represents a specific frequency and has infinite spatial extent\n",
    "\n",
    "* Wavelets Basis Functions, representing both space and frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial Basis Functions\n",
    "x=np.linspace(-1,1,1000)\n",
    "polynomials=np.array([x**i for i in range(10)]).T\n",
    "\n",
    "plt.plot(x, polynomials)\n",
    "plt.legend(range(10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>Feature Space</b> \n",
    "\n",
    "The set of features drawn by the transformation \n",
    "\n",
    "\\begin{align}\n",
    "\\phi: \\mathbb{R} & \\rightarrow \\mathbb{R}^M \\\\\n",
    "x & \\rightarrow [\\phi_1(x), \\phi_2(x), ..., \\phi_M(x)]\n",
    "\\end{align}\n",
    "is often called the **feature space**.\n",
    "When we write a linear regression with respect to a set of basis functions, the regression model is linear in the *feature space*.\n",
    "\n",
    "$M$ is dimensionality of the feature space and is often called the *model order*.\n",
    "</div>\n",
    "\n",
    "* Now, we want to find the mapping from the feature input data $\\mathbf{X}$ to the desired output values $\\mathbf{t}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose the data actually comes from some **unknown hidden function**, that takes in the data points $\\mathbf{x}$ with some parameters $\\mathbf{w}$ and produces the desired values $\\mathbf{t}$, i.e. $\\mathbf{t} = f(\\mathbf{x},\\mathbf{w})$.\n",
    "* We do not know anything about the function $f$. If we knew the hidden function, we would not need to learn the *mapping* - we would already know it. However, since we do not know the true underlying function, we need to do our best to estimate from the examples of input-output pairs that we have.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Step 3 - Model Selection or Mapping</b> \n",
    "\n",
    "Let's assume that the desired output values are a *linear combination* of the feature input space, i.e., the **polynomial function**\n",
    "\n",
    "$$t \\sim y(x,\\mathbf{w}) = w_0x^0 + w_1x^1 + w_2x^2+\\cdots+w_Mx^{M-1} = \\sum_{j=0}^{M-1} w_jx^j$$\n",
    "</div>\n",
    "\n",
    "* This means that for every paired training data point $\\{x_i, t_i\\}_{i=1}^N$, we can model the output value as \n",
    "\n",
    "$$t_i \\sim y(x_i,\\mathbf{w}) = w_0x_i^0 + w_1x_i^1 + w_2x_i^2+\\cdots+w_{M-1}x_i^{M-1} $$\n",
    "\n",
    "* Although the polynomial function $y(x,\\mathbf{w})$ is a nonlinear function of $x$, it is a linear function of the coefficients $\\mathbf{w}$. Functions, such as the polynomial, which are linear in the unknown parameters have important properties and are called *linear models*.\n",
    "\n",
    "The **linear basis model** for regression takes linear combinations of fixed nonlinear functions of the input variables\n",
    "\n",
    "$$t \\sim y(x,\\mathbf{w}) = w_0 + \\sum_{j=1}^{M-1} w_j\\phi_j(x)$$\n",
    "\n",
    "where $\\phi(x)$ are known as *basis function*.\n",
    "\n",
    "* The parameter $w_0$ allows for any fixed offset in the data and is sometimes called a *bias* parameter (not to be confused with ‘bias’ in a statistical sense). \n",
    "\n",
    "* It is often convenient to define an additional dummy *basis function* $\\phi_0(x) = 1$ so that\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Linear Basis Model</b> \n",
    "\n",
    "The linear basis model for regression takes linear combinations of fixed nonlinear functions of the input variables\n",
    "$$t \\sim y(x,\\mathbf{w}) = \\sum_{j=0}^{M-1} w_j\\phi_j(x)$$\n",
    "where $\\mathbf{w} = \\left[w_{0}, w_{1}, \\ldots, w_{M-1}\\right]^T$, $\\mathbf{\\phi} = \\left[\\phi_{0}, \\phi_{1}, \\ldots, \\phi_{M-1}\\right]^T$ and we have a total of $M$ parameters.\n",
    "</div>\n",
    "\n",
    "* The example of the **polynomial regression** is a particular example of this model in which there is a single input variable $x$, and the basis functions take the form of powers of $x$ so that $\\phi(x)=x^j$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of the coefficients $\\mathbf{w}$ will be determined by *fitting* the polynomial to the training data. \n",
    "\n",
    "This can be done by minimizing an **objective function** (also defined as **cost function**, **error function**, or **loss function**) that measures the *misfit* between the function $y(x,\\mathbf{w})$, for any given value of $\\mathbf{w}$, and the training set data points $\\{x_i,t_i\\}_{i=1}^N$.\n",
    "\n",
    "* What is the model's *objective* or goal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "image/png": {
       "width": 400
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image('figures/LeastSquares.png',width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One simple choice for fitting the model is to consider the error function given by the sum of the squares of the errors between the predictions $y(x_i,\\mathbf{w})$ for each data point $x_i$ and the corresponding target values $t_i$, so that we minimize\n",
    "\n",
    "\\begin{align*} J(\\mathbf{w}) &= \\frac{1}{2} \\sum_{n=1}^N \\left(t_n - y(x_n,\\mathbf{w})\\right)^2 \\\\\n",
    "&= \\frac{1}{2} \\sum_{n=1}^N \\left(t_n - \\sum_{j=0}^{M-1} w_jx_n^j\\right)^2\n",
    "\\end{align*}\n",
    "\n",
    "Other objective functions can be considered, for example, the absolute value of the absolute error is often used for sparsity concerns. But we will come back to this topic and discuss it further.\n",
    "    \n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Step 4 - Objective Function</b> \n",
    "\n",
    "This is the measure we want to optimize, that is, minimize (if it is an error function) or maximize if it is a reward function. The objective function can take many forms, for example, the **least squares error** or **mean least squares error**.\n",
    "\n",
    "$$J(\\mathbf{w}) = \\frac{1}{2} \\sum_{n=1}^N \\left(t_n - y(x_n,\\mathbf{w})\\right)^2 = \\frac{1}{2}\\left\\Vert \\mathbf{t} - y(\\mathbf{x},\\mathbf{w}) \\right\\Vert^2_2$$\n",
    "\n",
    "</div>\n",
    "\n",
    "* This error function is minimizing the (Euclidean) *distance* of every point to the curve.\n",
    "\n",
    "* Note that $J(\\mathbf{w})$ is a scalar value.\n",
    "\n",
    "* This objective function is minimizing the (Euclidean) *distance* of every point to the curve.\n",
    "\n",
    "* We can write the error function compactly in matrix/vector form:\n",
    "\\begin{align*}\n",
    "J(\\mathbf{w}) &= \\frac{1}{2}\\left\\Vert \\mathbf{t} - y(\\mathbf{x},\\mathbf{w}) \\right\\Vert^2_2 \\\\\n",
    "&= \\frac{1}{2} \\left\\Vert \\mathbf{t} - \\mathbf{X}\\mathbf{w} \\right\\Vert^2_2\\\\\n",
    "&= \\frac{1}{2} \\left(\\mathbf{t}-\\mathbf{X}\\mathbf{w} \\right)^T \\left(\\mathbf{t}-\\mathbf{X}\\mathbf{w}\\right)\\\\\n",
    "\\text{where: } & \\mathbf{X} = \\left[\\begin{array}{ccccc}\n",
    "1 & x_{1} & x_{1}^{2} & \\cdots & x_{1}^{M}\\\\\n",
    "1 & x_{2} & x_{2}^{2} & \\cdots & x_{2}^{M}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "1 & x_{N} & x_{N}^{2} & \\cdots & x_{N}^{M}\n",
    "\\end{array}\\right], \\mathbf{w} =  \\left[\\begin{array}{c}\n",
    "w_{0}\\\\\n",
    "w_{1}\\\\\n",
    "\\vdots\\\\\n",
    "w_{M}\n",
    "\\end{array}\\right], \\text{and }  \\mathbf{t} = \\left[\\begin{array}{c}\n",
    "t_{1}\\\\\n",
    "t_{2}\\\\\n",
    "\\vdots\\\\\n",
    "t_{N}\n",
    "\\end{array}\\right]\n",
    "\\end{align*}\n",
    "\n",
    "* Recall that $\\left\\Vert \\mathbf{z} \\right\\Vert_p$ is called the *p-norm* of the vector $\\mathbf{z}$ and is defined as $\\left(z_1^p + z_2^p + \\cdots + z_N^p\\right)^{\\frac{1}{p}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>Step 5 - Learning Algorithm</b> \n",
    "\n",
    "Also referred as training the model.\n",
    "\n",
    "We *fit* the polynomial function model such that the *error function* $J(\\mathbf{w})$ is minimized, i.e. we *optimize* the following error function\n",
    "$$J(\\mathbf{w}) = \\frac{1}{2}\\left\\Vert \\mathbf{t} - \\mathbf{X}\\mathbf{w} \\right\\Vert^2_2$$\n",
    "\n",
    "The optimization function is then:\n",
    "$$\\arg_{\\mathbf{w}}\\min J(\\mathbf{w})$$ \n",
    "</div>\n",
    "\n",
    "* We want to find the set of parameters $\\mathbf{w}$ that minimize the error function $J(\\mathbf{w})$.\n",
    "\n",
    "* What do you mean by **optimize** $J(\\mathbf{w})$? **How do you find $\\mathbf{w}$?**\n",
    "\n",
    "* We *optimize* the error function $J(\\mathbf{w})$ in order to find the *optimal* set of parameter $\\mathbf{w}^*$ that minimize the objective function.\n",
    "\n",
    "* To do that, we **take the derivative of $J(\\mathbf{w})$ with respect to the parameters $\\mathbf{w}$**.\n",
    "\n",
    "* How do you take the derivative of a *scalar*, such as $J(\\mathbf{w})$, with respect to a vector, such as $\\mathbf{w}$?\n",
    "\n",
    "    * What is the derivative of a scalar with respect to a vector?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The derivative of the scalar $J(\\mathbf{w})$ with respect to the vector $\\mathbf{w}=[w_0,w_1,\\dots,w_{M-1}]^T$ is a **vector**, and it corresponds to take the derivative of $J(\\mathbf{w})$ with respect to every element in $\\mathbf{w}$:\n",
    "\n",
    "$$\\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}} = \\left[ \\frac{\\partial J(\\mathbf{w})}{\\partial w_0},  \\frac{\\partial J(\\mathbf{w})}{\\partial w_1}, \\ldots,  \\frac{\\partial J(\\mathbf{w})}{\\partial w_{M-1}} \\right]^T$$\n",
    "\n",
    "* If we rewrite the objective function as:\n",
    "\\begin{align*}\n",
    "J(\\mathbf{w}) &= \\frac{1}{2} \\left(\\mathbf{t}- \\mathbf{X}\\mathbf{w}\\right)^T\\left(\\mathbf{t}- \\mathbf{X}\\mathbf{w}\\right) \\\\\n",
    "& = \\frac{1}{2} \\left(\\mathbf{t}^T - \\mathbf{w}^T\\mathbf{X}^T\\right)\\left(\\mathbf{t} - \\mathbf{X}\\mathbf{w} \\right) \\\\\n",
    "& = \\frac{1}{2} \\left( \\mathbf{t}^T\\mathbf{t} - \\mathbf{t}^T\\mathbf{X}\\mathbf{w} - \\mathbf{w}^T\\mathbf{X}^T \\mathbf{t} + \\mathbf{w}^T\\mathbf{X}^T\\mathbf{X}\\mathbf{w} \\right)\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "* Solving for $\\mathbf{w}$, we find:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}} &= 0 \\\\\n",
    "\\frac{\\partial }{\\partial \\mathbf{w}} \\left[\\frac{1}{2} \\left(\\mathbf{t}^T\\mathbf{t} - \\mathbf{t}^T\\mathbf{X}\\mathbf{w} - \\mathbf{w}^T\\mathbf{X}^T \\mathbf{t} + \\mathbf{w}^T\\mathbf{X}^T\\mathbf{X}\\mathbf{w} \\right) \\right] &= 0 \\\\\n",
    "\\frac{\\partial }{\\partial \\mathbf{w}} \\left[ \\left(\\mathbf{t}^T\\mathbf{t} - \\mathbf{t}^T\\mathbf{X}\\mathbf{w} - \\mathbf{w}^T\\mathbf{X}^T \\mathbf{t} + \\mathbf{w}^T\\mathbf{X}^T\\mathbf{X}\\mathbf{w} \\right) \\right] &= 0 \\\\\n",
    "- \\mathbf{t}^T\\mathbf{X} - (\\mathbf{X}^T \\mathbf{t})^T + (\\mathbf{X}^T\\mathbf{X}\\mathbf{w})^T + \\mathbf{w}^T\\mathbf{X}^T\\mathbf{X} &=0 \\\\\n",
    "\\mathbf{w}^T\\mathbf{X}^T\\mathbf{X} + \\mathbf{w}^T\\mathbf{X}^T\\mathbf{X} - \\mathbf{t}^T\\mathbf{X} - \\mathbf{t}^T\\mathbf{X} &= 0\\\\\n",
    "2 \\mathbf{w}^T\\mathbf{X}^T\\mathbf{X} &= 2 \\mathbf{t}^T\\mathbf{X} \\\\\n",
    "(\\mathbf{w}^T\\mathbf{X}^T\\mathbf{X})^T &= (\\mathbf{t}^T\\mathbf{X})^T\\text{, apply transpose on both sides} \\\\\n",
    "\\mathbf{X}^T\\mathbf{X}\\mathbf{w} &= \\mathbf{X}^T\\mathbf{t} \\\\\n",
    "\\mathbf{w} &= \\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\mathbf{t}\n",
    "\\end{align*}\n",
    "\n",
    "* This gives us the optimal set of parameters $\\mathbf{w}$ that minimize the objective function $J(\\mathbf{w})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Data**\n",
    "\n",
    "After the model is trained (i.e. complete optimization of error function using the training labeled data), the **goal** is to *predict* the output values to *new*, unseen and unlabeled test data.\n",
    "\n",
    "The steps in the test data are:\n",
    "* Step 1: Extract (the same) features\n",
    "* Step 2: Run through the trained model using the optimal set of parameters $\\mathbf{w}$ to computer the output prediction value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Implementation\n",
    "\n",
    "**What can you control?** \n",
    "\n",
    "<!-- * Model order -->\n",
    "<!-- * Feature vectors or *basis functions* -->\n",
    "\n",
    "How would you implement linear regression using polynomial features?\n",
    " * Let's see with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Suppose Input Data is sampled from a (noisy) sine curve \n",
    "\n",
    "Suppose our data comes from a noisy sinusoidal: $t = \\sin(2\\pi x) + \\epsilon$ where $\\epsilon$ is a (univariate) Gaussian zero-mean random noise. \n",
    "\n",
    "* The univariate Gaussian Distribution is defined as:\n",
    "$$\\mathcal{N}(x | \\mu, \\sigma^2) = \\frac{1}{(2\\pi \\sigma^2)^{1/2}} \\exp\\left\\{ - \\frac{(x - \\mu)^2}{2\\sigma^2}\\right\\}$$\n",
    "\n",
    "where $\\mu$ is the mean and $\\sigma^2$ is the variance. \n",
    "\n",
    "* If the noise is zero-mean Gaussian distributed, it is like we are saying there is a Gaussian around the true curve: \n",
    "\n",
    "$$t = y + \\epsilon$$\n",
    "\n",
    "Let's generate data from the *true* underlying function (which, in practice, we would not know)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NoisySinusoidalData(N, a, b, gVar):\n",
    "    '''Generates N data points in the range [a,b) sampled from a sin(2*pi*x) \n",
    "    with additive zero-mean Gaussian random noise with standard deviation gVar'''\n",
    "    # N input samples, evenly spaced numbers between [a,b) incrementing by 1/N\n",
    "    x = np.linspace(a,b,N)\n",
    "    # draw N sampled from a univariate Gaussian distribution with mean 0, gVar standard deviation and N data points\n",
    "    noise = np.random.normal(0,gVar,N) \n",
    "    # desired values, noisy sinusoidal\n",
    "    t = np.sin(2*np.pi*x) + noise\n",
    "    return x, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate input samples and desired values\n",
    "\n",
    "N = 50 # number of data samples\n",
    "a, b = [0,1] # data samples interval\n",
    "gVar1 = 0.3 # standard deviation of the zero-mean Gaussian noise\n",
    "x_train, t_train = NoisySinusoidalData(N, a, b, gVar1) # Training Data - Noisy sinusoidal\n",
    "x_true, t_true = NoisySinusoidalData(N, a, b, 0) # True Sinusoidal - in practice, we don't have the true function\n",
    "x_test, t_test = NoisySinusoidalData(20, a, b, 0.1) # Test Data - Noisy sinusoidal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(x_train, t_train, 'bo', label = 'Training Data')\n",
    "plt.plot(x_true, t_true, 'g', label = 'True Sinusoidal')\n",
    "plt.plot(x_test, t_test, 'ro', label = 'Test Data')\n",
    "plt.legend()\n",
    "plt.xlabel('Data Samples, x')\n",
    "plt.ylabel('Desired Values, t');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fit the data using the *polynomial regression* model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolynomialRegression(x,t,M):\n",
    "    '''Fit a polynomial of order M to the data input data x and desire values t'''\n",
    "    \n",
    "    # To be finished in class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Order\n",
    "M = 15\n",
    "\n",
    "# Find the parameters that fit the noisy sinusoidal\n",
    "w, y, error = PolynomialRegression(x_train,t_train,M) \n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(x_train,t_train,'bo', label='Training Data')\n",
    "plt.plot(x_train,y,'r', label = 'Estimated Polynomial')\n",
    "plt.plot(x_true,t_true,'g', label = 'True Function')\n",
    "plt.legend()\n",
    "plt.xlabel('Data Samples, x')\n",
    "plt.ylabel('Desired Values, t');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well does this trained model **generalize** to the test data, to which we do not have labels.\n",
    "\n",
    "In the test data:\n",
    "* Apply the same feature extraction as in training: $\\mathbf{X}_{test} = \\Phi(\\mathbf{x}_{test})$, where $\\mathbf{X}_{test}$ is a $K\\times (M+1)$ data matrix\n",
    "* Predict the output using $\\mathbf{w}^*$: $\\mathbf{y}_{test} = \\Phi(\\mathbf{x}_{test})\\mathbf{w}^*$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the parameters following the equation y = X*w or y = Phi * w in the Polynomial Regression case\n",
    "\n",
    "# To be finished in class\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(x_train,t_train,'bo', label='Training Data')\n",
    "plt.plot(x_train,y,'r', label = 'Estimated Polynomial')\n",
    "plt.plot(x_true,t_true,'g', label = 'True Function')\n",
    "plt.plot(x_test,t_test,'ro', label = 'Test Data')\n",
    "plt.plot(x_test,yt,'k', label = 'Predictions')\n",
    "plt.legend()\n",
    "plt.xlabel('Data Samples, x')\n",
    "plt.ylabel('Desired Values, t');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How do the weights look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "# To be finished in class\n",
    "\n",
    "plt.xticks(np.arange(len(w)), ['$w_{'+str(i)+'}$' for i in range(len(w))],rotation=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What happens when the test points fall outside the range of what the model has *learned*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 4\n",
    "w, y, error = PolynomialRegression(x_train,t_train,M) \n",
    "\n",
    "x_test2, t_test2 = NoisySinusoidalData(20, 0, 1.5, 0.1)\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(x_train, t_train, 'bo', label = 'Training Data')\n",
    "plt.plot(x_true, t_true, '-g', label = 'True Sinusoidal')\n",
    "plt.plot(x_test2, t_test2, 'ro', label = 'Test Data')\n",
    "plt.legend()\n",
    "plt.xlabel('Data Samples, x')\n",
    "plt.ylabel('Desired Values, t');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "# To be finished in class\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(x_train,t_train,'bo', label='Training Data')\n",
    "plt.plot(x_train,y,'r', label = 'Estimated Polynomial')\n",
    "plt.plot(x_true,t_true,'-g', label = 'True Function')\n",
    "plt.plot(x_test2,t_test2,'ro', label = 'Test Data')\n",
    "plt.plot(x_test2,yt2,'k', label = 'Test Predictions')\n",
    "plt.legend()\n",
    "plt.xlabel('Data Samples, x')\n",
    "plt.ylabel('Desired Values, t');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How do we select the *best* model order?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
