{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 10 - Basis Functions & Bayesian Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NoisySinusoidalData(N, a, b, gVar):\n",
    "    x = np.linspace(a,b,N)\n",
    "    noise = npr.normal(0,gVar,N)\n",
    "    t = np.sin(2*np.pi*x) + noise\n",
    "    return x, t\n",
    "\n",
    "Ntrain, Ntest = 100, 30\n",
    "a, b = [0,1] \n",
    "sigma_train, sigma_test = 0.3, 0.5\n",
    "x_train, t_train = NoisySinusoidalData(Ntrain, a, b, sigma_train)    # training data and labels\n",
    "x_true, t_true = NoisySinusoidalData(Ntrain, a, b, 0)             #true sine function\n",
    "x_test, t_test = NoisySinusoidalData(Ntest, a, b, sigma_test) # test data and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basis Functions\n",
    "\n",
    "So far, we have assumed that we have form a **feature matrix** $\\mathbf{X}$ of dimensions $N \\times M$, where $N$ is the number of samples and $M$ the number of coefficients.\n",
    "\n",
    "* In Polynomial Regression, we constructed this matrix with a polynomial representation of each data sample:\n",
    "\n",
    "$$\\phi(x_i) = [x_i^0, x_i^1, \\cdots, x_i^p]^T$$\n",
    "\n",
    "In practice, we can use other types of features. But, from a software implementation point-of-view, regardless of the features used, we always want to have them in a tidy feature matrix $\\mathbf{X}$:\n",
    "\n",
    "$$\\mathbf{X} =\\left[\\begin{array}{c} \\phi(x_1)^T \\\\ \\phi(x_2)^T \\\\ \\vdots \\\\ \\phi(x_N)^T \\end{array}\\right]  = \\left[\\begin{array}{ccccc}\n",
    "1 & x_{1} & x_{1}^{2} & \\cdots & x_{1}^{p}\\\\\n",
    "1 & x_{2} & x_{2}^{2} & \\cdots & x_{2}^{p}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "1 & x_{N} & x_{N}^{2} & \\cdots & x_{N}^{p}\n",
    "\\end{array}\\right] \\in \\mathbb{R}^{N\\times M}$$\n",
    "\n",
    "where $M=p+1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In mathematics, the polynomial feature representation that we have used is called a **polynomial basis function** and is a basis of a polynomial ring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can also implement the **Linear Regression** model using other feature representations, also called **basis functions** $\\phi(\\mathbf{x})$. \n",
    "\n",
    "    * **Assumption:** we are assuming that true *unknown* function $f(x)$ can be modeled by at least one of the functions $\\phi(\\mathbf{x})$ that can be represented by a linear combination of the basis functions, i.e., by one function in the function class under consideration.\n",
    "    \n",
    "    * If we include **too few** basis functions or unsuitable basis functions, we might not be able to model the true dependency. Similarly to polynomial features, the more we added, the better fit we have to out training data.\n",
    "    \n",
    "    * If we include **too many** basis functions, we need many data points to fit all the unknown parameters.\n",
    "        * There are special function spaces such as the Reproducing Kernel Hilbert Space or RKHS, where we can have an *infinite* number of basis functions (called kernels) and still have good generalization.\n",
    "    \n",
    "    * What we control? Which basis functions to use, how many to use, and any other parameters they have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples include:\n",
    "\n",
    "1. **Radial Basis Functions (RBF)**\n",
    "\n",
    "Another popular class of basis functions are radial basis functions (RBF). Typical representatives are Gaussian basis functions:\n",
    "\n",
    "$$\\phi_j(x) = \\exp\\left\\{-\\frac{(x-\\mu_j)^2)}{2s_j^2}\\right\\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RBF(x,m,s):\n",
    "    return np.exp(- ((x-m)**2)/(2*s**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 10\n",
    "m = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "s = [0.05]*10\n",
    "\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.plot(x_train,t_train,'bo', label='Training Data')\n",
    "plt.plot(x_true, t_true,'g',linewidth=5, label = 'True Function')\n",
    "for i in range(M):\n",
    "    plt.plot(x_train, RBF(x_train,m[i],s[i]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given these basis functions, we need to:\n",
    "1. compute the weight value that each one has in representing the underlying function $f(x)$ - these are the coefficient $\\mathbf{w}$\n",
    "2. estimate the parameters of each basis function, in particular, the mean $\\mu$ and standard deviation $\\sigma$ for each Gaussian basis function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example from textbook \"Python Data Science Handbook\"\n",
    "# https://jakevdp.github.io/PythonDataScienceHandbook/05.06-linear-regression.html#Gaussian-Basis-Functions\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class GaussianFeatures(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Uniformly-spaced Gaussian Features for 1D input\"\"\"\n",
    "    \n",
    "    def __init__(self, N, width_factor=2.0):\n",
    "        self.N = N\n",
    "        self.width_factor = width_factor\n",
    "    \n",
    "    @staticmethod\n",
    "    def _gauss_basis(x, y, width, axis=None):\n",
    "        arg = (x - y) / width\n",
    "        return np.exp(-0.5 * np.sum(arg ** 2, axis))\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # create N centers spread along the data range\n",
    "        self.centers_ = np.linspace(X.min(), X.max(), self.N)\n",
    "        self.width_ = self.width_factor * (self.centers_[1] - self.centers_[0])\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return self._gauss_basis(X[:, :, np.newaxis], self.centers_,\n",
    "                                 self.width_, axis=1)\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "x = 10 * rng.rand(50)\n",
    "y = np.sin(x) + 0.1 * rng.randn(50) # Noisy sinusoidal curve\n",
    "xfit = np.linspace(0, 10, 1000)\n",
    "\n",
    "gauss_model = make_pipeline(GaussianFeatures(10, 1.0),\n",
    "                            LinearRegression())\n",
    "gauss_model.fit(x[:, np.newaxis], y)\n",
    "yfit = gauss_model.predict(xfit[:, np.newaxis])\n",
    "\n",
    "gf = gauss_model.named_steps['gaussianfeatures']\n",
    "lm = gauss_model.named_steps['linearregression']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,5))\n",
    "\n",
    "for i in range(10):\n",
    "    selector = np.zeros(10)\n",
    "    selector[i] = 1\n",
    "    Xfit = gf.transform(xfit[:, None]) * selector\n",
    "    yfit = lm.predict(Xfit)\n",
    "    ax.fill_between(xfit, yfit.min(), yfit, color='gray', alpha=0.2)\n",
    "\n",
    "ax.scatter(x, y,label='Training Data')\n",
    "ax.plot(xfit, gauss_model.predict(xfit[:, np.newaxis]),'r',label='Estimated Model')\n",
    "ax.plot(np.sort(x), np.sin(np.sort(x)), 'g',label='Sinusoidal Curve')\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(yfit.min(),2.5)\n",
    "plt.legend(fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will study this into more detail when introducing **Gaussian Mixture Models (GMMs)**, but we are not quite ready yet..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other basis functions include:\n",
    "\n",
    "2. **Fourier Basis functions**\n",
    "\n",
    "3. **Wavelet Basis functions**\n",
    "\n",
    "4. and many others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Interpretation - The Evidence Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the **Regularized Least Squares** in the \"Objective Function world\", where we simply add a term to our objective in order to prevent overfitting and, consequently, allow the model to generalize to unseen and unkown data.\n",
    "\n",
    "Another way to look at Regularized Least Squares is from a Bayesian point-of-view. To see this, let's look at our objective function:\n",
    "\n",
    "$$J(\\mathbf{w})= \\frac{1}{2N}\\sum_{n=1}^N \\left(t_n - y_n\\right)^2 + \\frac{\\lambda}{2} \\sum_{j} w_j^2$$\n",
    "\n",
    "Now, if we optimize for a solution for $\\mathbf{w}$, we have\n",
    "\n",
    "\\begin{align}\n",
    "& \\arg_{\\mathbf{w}}\\min \\left(J(\\mathbf{w})\\right) \\\\\n",
    "= & \\arg_{\\mathbf{w}}\\max \\left(- J(\\mathbf{w})\\right) \\\\\n",
    "= & \\arg_{\\mathbf{w}}\\max \\left(\\exp\\left(- J(\\mathbf{w})\\right)\\right) \\text{, }\\exp(\\bullet)\\text{ is a monotonic function}  \n",
    "\\end{align}\n",
    "\n",
    "Substituting,\n",
    "\n",
    "\\begin{align}\n",
    "& \\arg_{\\mathbf{w}}\\max \\left(\\exp\\left(-\\frac{1}{2N}\\sum_{n=1}^N \\left(t_n - y_n\\right)^2 - \\frac{\\lambda}{2} \\sum_{j} w_j^2)\\right)\\right) \\\\\n",
    "\\propto & \\arg_{\\mathbf{w}}\\max \\left(\\exp\\left(-\\frac{1}{2}\\sum_{n=1}^N \\left(t_n - y_n\\right)^2 - \\frac{\\lambda}{2} \\sum_{j} w_j^2)\\right)\\right) \\\\\n",
    "= & \\arg_{\\mathbf{w}}\\max \\left(\\exp\\left(-\\frac{1}{2}\\sum_{n=1}^N \\left(t_n - y_n\\right)^2\\right) \\exp\\left(- \\frac{\\lambda}{2} \\sum_{j} w_j^2)\\right)\\right) \\\\\n",
    "=& \\arg_{\\mathbf{w}}\\max \\left(\\prod_{n=1}^N \\exp\\left(-\\frac{1}{2}\\left(t_n - y_n\\right)^2\\right) \\prod_{j} \\exp \\left(-\\frac{\\lambda}{2} w_j^2\\right) \\right)  \\\\\n",
    "\\approx & \\arg_{\\mathbf{w}}\\max G\\left(\\mathbf{t}|\\mathbf{y}, 1\\right) G\\left(\\mathbf{w}|0, 1/\\lambda\\right)\\text{, assuming data }\\{(x_i,t_i)\\}_{i=1}^N\\text{ is i.i.d.} \\\\\n",
    "=& \\arg_{\\mathbf{w}}\\max P(\\mathbf{t}|\\mathbf{w}) P(\\mathbf{w}), \\mathbf{y} \\text{ is a function of }\\mathbf{w}\\\\\n",
    "=& \\arg_{\\mathbf{w}}\\max P(\\mathbf{w}|\\mathbf{t}) P(\\mathbf{t}), \\text{ using Bayes' Theorem} \\\\\n",
    "\\propto & \\arg_{\\mathbf{w}}\\max P(\\mathbf{w}|\\mathbf{t}), \\text{because } P(\\mathbf{t})\\text{ is constant for some fixed training set}  \n",
    "\\end{align}\n",
    "\n",
    "where $P(\\mathbf{t}|\\mathbf{w})$ is the **data likelihood** of the target label vector $\\mathbf{t}$ given some coefficients $\\mathbf{w}$, $P(\\mathbf{w})$ is the **prior** on the parameters, and $P(\\mathbf{w}|\\mathbf{t})$ is the **posterior probability**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Machine Learning, this result is known as the **evidence approximation**.\n",
    "\n",
    "* In practice, this means that we now can rewrite the Regularized Least Squares problem as the product between the *data likelihood* and a *prior distribution* on the parameters. \n",
    "\n",
    "    * In particular, for Least Squares cost function and an L2- regularization term, both data likelihood and prior distributions are Gaussian-distributed.\n",
    "    \n",
    "* Now, we can select **any** distribution function to our data and control the regularization also using a probabilistic model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the shape of the prior distribution if we had considered the L1-norm or the Lasso regularizer?**\n",
    "\n",
    "* Recall that, if $X$ is Laplacian distributed with parameters $\\mu$ and $b$ ($b>0$), its probability density function (pdf) is defined as: \n",
    "\n",
    "$$f_X(x) = \\frac{1}{2b}\\exp\\left\\{-\\frac{|x-\\mu|}{b}\\right\\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-4,4,1000)\n",
    "Gaussian = np.exp(-x**2/2)/np.sqrt(2*np.pi) #Gaussian with zero-mean and unit-variance\n",
    "Laplacian = np.exp(-np.abs(x))/(2) #Laplacian with zero-mean and lambda=1\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(x, Gaussian, '--', label='$\\mathcal{N}(\\mu=0,\\sigma^2=1)$')\n",
    "plt.plot(x, Laplacian, label='$\\mathcal{L}(\\mu=0,b=1/\\lambda)$')\n",
    "plt.legend(loc='best', fontsize=15)\n",
    "plt.xlabel('$\\mathbf{w}$', size=15)\n",
    "plt.ylabel('Prior Probability\\n $P(\\mathbf{w})$', size=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation (MLE) & Maximum A Posteriori (MAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that our goal is to find the set of hyper-parameters that best fit our data. \n",
    "\n",
    "With the evidence approximation, rather than integrating over all possible values, we can use Bayesian inferencing to find the set of coefficients that maximize a distribution.\n",
    "\n",
    "For the **Regularized Least Squares** objective function, we just showed that our optimization problem can be reduced to:\n",
    "\n",
    "* Maximizing the **posterior** probability, that takes the shape of a Gaussian distribution, of unknown hyper-parameters.\n",
    "\n",
    "For the **Least Squares without regularization** objective function, we just showed that our optimization problem can be reduced to:\n",
    "\n",
    "* Maximizing the **data likelihood**, that takes the shape of a Gaussian distribution, with unknown hyper-parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our problem, the hypothesis are the *unknown* **hyper-parameters** $\\mathbf{w}$.\n",
    "\n",
    "* In Bayesian statistical inferencing, we are then trying to find the $\\mathbf{w}$'s that maximizing the posterior probability.\n",
    "* In classical statistical inferencing, on the other hand, we are only computing the probability of some hypothesis (the *null hypothesis*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<h2 align=\"center\"><span style=\"color:blue\">Maximum Likelihood Estimation (MLE)</span></h2>\n",
    "<center>(Frequentist approach)</center>\n",
    "\n",
    "In **Maximum Likelihood Estimation** (also referred to as **MLE**) we want to *find the set of parameters* that **maximize** the data likelihood $P(\\mathbf{x}|\\mathbf{w})$. We want to find the *optimal* set of parameters under some assumed distribution such that the data is most likely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<h2 align=\"center\"><span style=\"color:orange\">Maximum A Posteriori (MAP)</span></h2>\n",
    "<center>(Bayesian approach)</center>\n",
    "\n",
    "In **Maximum A Posteriori** (also referred as **MAP**) we want to *find the set of parameters* that **maximize** the posteriori probability $P(\\mathbf{w}|\\mathbf{x})$. We want to find the *optimal* set of parameters under some assumed distribution such that the parameters are most likely to have been drawn off of given some prior beliefs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "**Suppose you flip a coin 3 times and observe the event H-H-H. What is the probability that the next flip is also Heads (H)?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. From Classical probability, what is the probability of heads in the next flip?\n",
    "\n",
    "    * $P(H) = \\frac{\\text{# observed Heads}}{\\text{# flips}} = \\frac{3}{3} = 1$\n",
    "\n",
    "2. Bayesian Inference: What is the **hidden state** in this problem?\n",
    "\n",
    "    * Hidden state: what type of coin was use in the experiment\n",
    "    * Let's consider there could been two types of coins in play: 1 fair coin and 1 2-headed coin\n",
    "    * So, by the Law of Total Probability:\n",
    "    \n",
    "    $$P(H) = P(H|\\text{fair})P(\\text{fair}) + P(\\overline{H}|\\text{2-headed})P(\\text{2-headed})$$\n",
    "    * Furthermore, we can test different hypothesis by checking which hypothesis has the largest posterior probability value, e.g. if $P(\\text{fair}|E) > P(\\text{2-headed}|E)$, then hypothesis \"fair\" is more likely and that is what we will use to make predictions.\n",
    "    \n",
    "3. Let $H_i$ be the event of observing Heads on flip $i$. Note that the events $H_i$ are **conditionally independent**, that is: $P(H_1\\cap H_2|\\text{fair}) = P(H_1|\\text{fair})P(H_2|\\text{fair})$. But they are **not** (necessarily) statistically independent.\n",
    "    * This is often an assumption that we make about data samples, we say that the samples are **independent and identically distributed (i.i.d.)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's consider heads=1 and tails=0, so our sample space is $S=\\{1,0\\}$. Now, let the probability of heads is equal to some *unknown* value $\\mu$, then:\n",
    "\n",
    "\\begin{align}\n",
    "& P(x=1 | \\mu) = \\mu \\\\\n",
    "& P(x=0|\\mu) = 1-\\mu\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the data likelihood as:\n",
    "\n",
    "$$P(x|\\mu) = \\mu^x(1-\\mu)^{1-x} = \\begin{cases}\\mu & \\text{if }x=1 \\\\ 1-\\mu & \\text{if } x=0 \\end{cases}$$\n",
    "\n",
    "* This is the **Bernoulli distribution**. The mean and variance of the Bernoulli distribution are: $E[x] = \\mu$ and $E[\\left(x- E[x]\\right)^2] = \\mu(1-\\mu)$.\n",
    "\n",
    "* So, for every outcome of the event $E$, we will model it using a Bernoulli distribution, and each outcome is pairwise **conditionally independent**. Therefore, we have the event $E$ contains i.i.d. outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Maximum Likelihood Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity of calculation, let $E=x_1\\cap x_2\\cap \\dots\\cap x_N$, where $x_i=\\{0,1\\}$ (0 for Tails and 1 for Heads). Then, for an experiment with $N$ samples, we can write the **data likelihood** as:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "P(E|\\mu) &= P(x_1\\cap x_2\\cap \\dots\\cap x_N|\\mu) \\\\\n",
    "&= P(x_1|\\mu)P(x_2|\\mu)\\dots P(x_N|\\mu) \\\\\n",
    "&= \\prod_{n=1}^N P(x_n|\\mu) \\\\\n",
    "&= \\prod_{n=1}^N \\mu^{x_n} (1-\\mu)^{1-x_n}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now, we are interested in finding the value of $\\mu$ given some data set $E$. \n",
    "\n",
    "We now optimize the data likelihood. What trick can we use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$arg_\\mathbf{\\mu} \\max P(E|\\mu) = \\arg_\\mathbf{\\mu} \\max \\ln \\left( P(E|\\mu) \\right)$$\n",
    "\n",
    "because the $\\ln(\\bullet)$ is a monotonic function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where \n",
    "$$\\ln(P(E|\\mu) = \\sum_{n=1}^N \\left(x_n \\ln(\\mu) + (1-x_n)\\ln(1-\\mu)\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we can take the derivative of this function with respect to (w.r.t.) $\\mu$ and equal it to zero:\n",
    "\n",
    "$$\\frac{\\partial \\ln(P(E|\\mu))}{\\partial \\mu} = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "(1-\\mu)\\sum_{n=1}^N x_n - \\mu \\left(N - \\sum_{n=1}^N x_n\\right) &= 0 \\\\\n",
    "\\sum_{n=1}^N x_n - \\mu\\sum_{n=1}^N x_n - \\mu N + \\mu\\sum_{n=1}^N x_n &= 0 \\\\\n",
    "\\sum_{n=1}^N x_n - \\mu N &= 0 \\\\\n",
    "\\mu &= \\frac{1}{N} \\sum_{n=1}^N x_n\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, *as expcted*, the MLE estimation for the probability of seeing heads in the next coin flip is equal to **relative frequency** of the outcome heads.\n",
    "\n",
    "* Suppose you flipped the coin only once, and saw Tails. The probability of flipping Heads according to MLE would be 0.\n",
    "\n",
    "* MLE is **purely data driven**! This is sufficient *when* we have lots and lots of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Maximum A Posteriori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the MAP estimation of $\\mu$, we are instead optimizing the posterior probability:\n",
    "\n",
    "\\begin{align}\n",
    "&\\arg_{\\mu} \\max P(\\mu|E) \\\\\n",
    "=& \\arg_{\\mu} \\max \\frac{P(E|\\mu) P(\\mu)}{P(E)} \\\\\n",
    "\\propto & \\text{  } \\arg_{\\mu} \\max P(E|\\mu) P(\\mu), P(E)\\text{ is some constant value} \n",
    "\\end{align}\n",
    "\n",
    "We have defined the data likelihood $P(E|\\mu)$, we now need to choose a **prior distribution** $P(\\mu)$.\n",
    "\n",
    "* This prior distribution will *encode* any prior knowledge we have about the hidden sate of the problem, in this case, the type of coin that was used.\n",
    "\n",
    "Let's say our **prior distribution** is a Beta Distribution. A **Beta Distribution** takes the form:\n",
    "\n",
    "$$\\text{Beta}(x|\\alpha,\\beta) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} x^{\\alpha-1} (1-x)^{\\beta-1}$$\n",
    "\n",
    "where $\\Gamma(x) = (x-1)!$ and $\\alpha,\\beta>0$.\n",
    "\n",
    "The mean and variance of the Beta distribution are: $E[x] = \\frac{\\alpha}{\\alpha+\\beta}$ and $E[(x-E[x])^2] = \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's see what that looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "a = 2\n",
    "b = 10\n",
    "x = np.arange(0,1,0.0001)\n",
    "Beta = (math.gamma(a+b)/(math.gamma(a)*math.gamma(b)))*x**(a-1)*(1-x)**(b-1)\n",
    "\n",
    "plt.plot(x, Beta, label='Beta Distribution')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Probability of Heads\\n $\\mu$',fontsize=15)\n",
    "plt.ylabel('Prior Probability\\n $P(\\mu)$',fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Beat Distribution as out prior, we have:\n",
    "\n",
    "\\begin{align}\n",
    "P(\\mu|\\alpha,\\beta) &= \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\mu^{\\alpha-1} (1-\\mu)^{\\beta-1} \\\\\n",
    "&\\propto \\mu^{\\alpha-1} (1-\\mu)^{\\beta-1}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let:\n",
    "* $m$ the number of heads\n",
    "* $l$ the number of tails\n",
    "* $N=m+l$ the total number of coin flips \n",
    "\n",
    "We can write our **posterior probability** as:\n",
    "\n",
    "\\begin{align}\n",
    "P(\\mu|E) &= \\frac{P(E|\\mu)P(\\mu)}{P(E)}\\\\\n",
    "&\\propto P(E|\\mu)P(\\mu)\\\\\n",
    "&= \\left(\\prod_{n=1}^N \\mu^{x_n} (1-\\mu)^{1-x_n}\\right) \\mu^{\\alpha-1} (1-\\mu)^{\\beta-1} \\\\\n",
    "&= \\mu^m (1-\\mu)^l \\mu^{\\alpha-1} (1-\\mu)^{\\beta-1} \\\\\n",
    "&= \\mu^{m+\\alpha-1} (1-\\mu)^{l+\\beta-1}\n",
    "\\end{align}\n",
    "\n",
    "* The posterior probability has the same shape as the data likelihood. \n",
    "\n",
    "* This is a special case called **Conjugate Prior Relationship**, which happens when the posterior has the same form as the prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now optimize our posterior probability, and we will apply the same trick:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$arg_\\mathbf{\\mu} \\max P(\\mu|E) = \\arg_\\mathbf{\\mu} \\max \\ln \\left( P(\\mu|E) \\right)$$\n",
    "\n",
    "where\n",
    "\n",
    "$$ \\ln \\left( P(\\mu|E) \\right) =  (m+\\alpha-1)\\ln(\\mu) + (l+\\beta-1)\\ln(1-\\mu)$$\n",
    "\n",
    "We can now *optimize* our posterior probability:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial  \\ln \\left( P(\\mu|E) \\right)}{\\partial \\mu} &= 0\\\\\n",
    "\\frac{m+\\alpha-1}{\\mu} + \\frac{l+\\beta-1}{1-\\mu} &= 0\\\\\n",
    "\\mu &= \\frac{m+\\alpha-1}{m + l + \\alpha + \\beta -2}\n",
    "\\end{align}\n",
    "\n",
    "This is our estimation of the probability of heads using MAP!\n",
    "\n",
    "* Our estimation for the probability of heads, $\\mu$, is going to depend on $\\alpha$ and $\\beta$ introduced by the prior distribution. We saw that they control the level of certainty as well as the center value.\n",
    "\n",
    "* With only a few samples, the prior will play a bigger role in the decision, but eventually the data takes over the prior.\n",
    "\n",
    "Let's run a simulation to compare MAP and MLE estimators."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
