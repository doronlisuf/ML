{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from scipy.stats import multivariate_normal\n",
    "import textwrap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_prior_func(beta = 2, alpha = 1, draw_num=(0,1,10,20,50,100)):\n",
    "    '''Online Update of the Posterior distribution for a Gaussian-Gaussian conjugate prior.\n",
    "    Parameter:\n",
    "    beta - variance of the data likelihood (of the additive noise)\n",
    "    alpha - precision value or 1/variance of the prior distribution\n",
    "    draw_num - number of points collected at each instance.\n",
    "    \n",
    "    This function will update the prior distribution as new data points are received.\n",
    "    The prior distribution will be the posterior distribution from the last iteration.'''\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 20), dpi= 80, facecolor='w', edgecolor='k')\n",
    "\n",
    "    # true (unknown) weights\n",
    "    a = -0.3 # w0\n",
    "    b = 0.5  # w1\n",
    "    \n",
    "    # set up input space\n",
    "    rangeX = [-2, 2] # range of values for the input\n",
    "    step = 0.025 # distance between points\n",
    "    X = np.mgrid[rangeX[0]:rangeX[1]:step] # creates a grid of values for input samples\n",
    "\n",
    "    #initialize prior/posterior and sample data\n",
    "    S0 = (1/alpha)*np.eye(2) # prior covariance matrix\n",
    "    sigma = S0 # copying it so we can update it later\n",
    "    mean = [0,0] # mean for prior\n",
    "    \n",
    "    # Draws samples from Uniform(-1,1) distribution\n",
    "    draws = np.random.uniform(rangeX[0],rangeX[1],size=draw_num[-1])\n",
    "    # Generate the noisy target samples\n",
    "    T = a + b*draws + np.random.normal(loc=0, scale=math.sqrt(beta))\n",
    "\n",
    "    for i in range(len(draw_num)):\n",
    "        if draw_num[i]>0: #skip first image\n",
    "            \n",
    "            # INPUT DATA\n",
    "            #Feature Matrix (Polynomial features with M=2)\n",
    "            FeatureMatrix = np.array([draws[:draw_num[i]]**m for m in range(2)]).T\n",
    "            #Target Values\n",
    "            t = T[0:draw_num[i]]\n",
    "            \n",
    "            # POSTERIOR PROBABILITY\n",
    "            # Covariance matrix\n",
    "            sigma = np.linalg.inv(S0 + beta*FeatureMatrix.T@FeatureMatrix)\n",
    "            # Mean vector\n",
    "            mean = beta*sigma@FeatureMatrix.T@t\n",
    "            \n",
    "            # PARAMETER SPACE\n",
    "            # create a meshgrid of possible values for w's\n",
    "            w0, w1 = np.mgrid[rangeX[0]:rangeX[1]:step, rangeX[0]:rangeX[1]:step]\n",
    "            \n",
    "            # Define the Gaussian distribution for data likelihood\n",
    "            p = multivariate_normal(mean=t[draw_num[i]-1], cov=beta)\n",
    "            # Initialize the PDF for data likelihood\n",
    "            out = np.empty(w0.shape)\n",
    "            # For each value (w0,w1), compute the PDF for all data samples\n",
    "            for j in range(len(w0)):\n",
    "                out[j] = p.pdf(w0[j]+w1[j]*draws[draw_num[i]-1])\n",
    "            \n",
    "            # Plot the data likelihood\n",
    "            ax = fig.add_subplot(*[len(draw_num),3,(i)*3+1])\n",
    "            ax.pcolor(w0, w1, out)\n",
    "            # Add the current value for parameters w=(w0,w1)\n",
    "            ax.scatter(a,b, c='c')\n",
    "            myTitle = 'data likelihood'\n",
    "            ax.set_title(\"\\n\".join(textwrap.wrap(myTitle, 100)))\n",
    "\n",
    "        # PARAMETER SPACE\n",
    "        # create a meshgrid of possible values for w's\n",
    "        w0, w1 = np.mgrid[rangeX[0]:rangeX[1]:step, rangeX[0]:rangeX[1]:step]\n",
    "        \n",
    "        # POSTERIOR PROBABILITY\n",
    "        # initialize the matrix with posterior PDF values\n",
    "        pos = np.empty(w1.shape + (2,))\n",
    "        # for w0\n",
    "        pos[:, :, 0] = w0\n",
    "        # and for w1\n",
    "        pos[:, :, 1] = w1\n",
    "        # compute the PDF\n",
    "        p = multivariate_normal(mean=mean, cov=sigma)\n",
    "\n",
    "        #Show prior/posterior\n",
    "        ax = fig.add_subplot(*[len(draw_num),3,(i)*3+2])\n",
    "        ax.pcolor(w0, w1, p.pdf(pos))\n",
    "        # Add the value for parameters w=(w0,w1) that MAXIMIZE THE POSTERIOR\n",
    "        ax.scatter(a,b, c='c')\n",
    "        myTitle = 'Prior/Posterior'\n",
    "        ax.set_title(\"\\n\".join(textwrap.wrap(myTitle, 100)))\n",
    "\n",
    "        # DATA SPACE\n",
    "        ax = fig.add_subplot(*[len(draw_num),3,(i)*3+3])\n",
    "        for j in range(6):\n",
    "            # draw sample from the prior probability to generate possible values for parameters \n",
    "            w0, w1 = np.random.multivariate_normal(mean=mean, cov=sigma)\n",
    "            # Estimated labels\n",
    "            t = w0 + w1*X\n",
    "            # Show data space\n",
    "            ax.plot(X,t)\n",
    "            if draw_num[i] > 0:\n",
    "                ax.scatter(FeatureMatrix[:,1], T[0:draw_num[i]])\n",
    "            myTitle = 'data space'\n",
    "            ax.set_title(\"\\n\".join(textwrap.wrap(myTitle, 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# beta - variance of the data likelihood (for the additive noise)\n",
    "# alpha - precision value or 1/variance for the prior distribution\n",
    "# draw_num - number of points collected at each instance\n",
    "\n",
    "likelihood_prior_func(beta = 0.5, alpha = 1/2, draw_num=(0,1,2,20,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 14 - Probabilistic Generative Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have training data from two classes, $C_1$ and $C_2$, and we would like to train a classifier to assign a label to incoming test points whether they belong to class $C_1$ or $C_2$.\n",
    "\n",
    "There are *many* classifiers in the machine learning literature. We will cover a few in this course. Today we will focus on probabilistic generative approaches for classification.\n",
    "\n",
    "* There are two types of probabilistic models: **discriminative** and **generative**.\n",
    "\n",
    "A **discriminative** approach for classification is one in which we partition the feature space into regions for each class. Then, when we have a test point, we evaluate in which region it landed on and classify it accordingly.\n",
    "\n",
    "A **generative** approach for classification is one in which we estimate the parameters for distributions that generate the data for each class. Then, when we have a test point, we can compute the posterior probability of that point belonging to each class and assign the point to the class with the highest posterior probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateData(mean1, mean2, cov1, cov2, N1, N2):\n",
    "    # We are generating data from two Gaussians to represent two classes\n",
    "    # In practice, we would not do this - we would just have data from the problem we are trying to understand\n",
    "    data_C1 = np.random.multivariate_normal(mean1, cov1, N1)\n",
    "    data_C2 = np.random.multivariate_normal(mean2, cov2, N2)\n",
    "    \n",
    "    plt.scatter(data_C1[:,0], data_C1[:,1], c='r')\n",
    "    plt.scatter(data_C2[:,0], data_C2[:,1])\n",
    "    plt.xlabel('Feature 1'); plt.ylabel('Feature 2')\n",
    "    plt.axis([-4,4,-4,4])\n",
    "    return data_C1, data_C2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean1 = [-1, -1]\n",
    "mean2 = [1, 1]\n",
    "cov1 = [[1,0],[0,1]]\n",
    "cov2 = [[1,0],[0,1]]\n",
    "N1 = 50\n",
    "N2 = 100\n",
    "\n",
    "data_C1, data_C2 = generateData(mean1, mean2, cov1, cov2, N1, N2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the data we generated above, we have a \"red\" class and a \"blue\" class. When we are given a test sample, we will want to assign the label of red or blue.\n",
    "\n",
    "We can compute the **posterior probability** for class $C_1$ as follows:\n",
    "\n",
    "$$P(C_1|x) = \\frac{P(x|C_1)P(C_1)}{P(x)}$$\n",
    "\n",
    "Understanding that the two classes, red and blue, form a partition of all possible classes, then we can utilize the *Law of Total Probability*, and obtain:\n",
    "\n",
    "$$P(C_1|x)=\\frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1) + P(x|C_2)P(C_2)}$$\n",
    "\n",
    "Similarly, we can compute the posterior probability for class $C_2$:\n",
    "\n",
    "$$P(C_2|x) = \\frac{P(x|C_2)P(C_2)}{P(x|C_1)P(C_1) + P(x|C_2)P(C_2)}$$\n",
    "\n",
    "Note that $P(C_1|x) + P(C_2|x) = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier\n",
    "\n",
    "Therefore, for a given test point $\\mathbf{x}^*$, our decision rule is:\n",
    "\n",
    "$$P(C_1|\\mathbf{x}^*) \\underset{C_2}{\\overset{C_1}{\\gtrless}} P(C_2|\\mathbf{x}^*)$$\n",
    "\n",
    "Using the Bayes' rule, we can further rewrite it as:\n",
    "\\begin{align*}\n",
    "\\frac{P(\\mathbf{x}^*|C_1)P(C_1)}{P(\\mathbf{x}^*)} &\\underset{C_2}{\\overset{C_1}{\\gtrless}} \\frac{P(\\mathbf{x}^*|C_2)P(C_2)}{P(\\mathbf{x}^*)}\n",
    "\\end{align*}\n",
    "\n",
    "This defines the **Naive Bayes Classifier**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Generative Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, **to train the classifier**, what we need to do is to determine the parametric form (and its parameters values) for $p(x|C_1)$, $p(x|C_2)$, $P(C_1)$ and $p(C_2)$.\n",
    "\n",
    "* For example, we can assume that the data samples coming from either $C_1$ and $C_2$ are distributed according to Gaussian distributions. In this case, \n",
    "\n",
    "$$p(x|C_k) = \\frac{1}{(2\\pi)^{1/2} |\\Sigma_k|^{1/2}}\\exp\\left\\{-\\frac{1}{2}(\\mathbf{x}-\\mathbf{\\mu}_k)^T\\Sigma_k^{-1}(\\mathbf{x}-\\mathbf{\\mu}_k)\\right\\}, \\forall k=\\{1,2\\}$$\n",
    "\n",
    "For this case, this defines the **Gaussian Naive Bayes Classifier**.\n",
    "\n",
    "Or, we can consider any other parametric distribution.\n",
    "\n",
    "* What about the $P(C_1)$ and $P(C_2)$?\n",
    "\n",
    "We can consider the relative frequency of each class, that is, $P(C_k) = \\frac{N_k}{N}$, where $N_k$ is the number of points in class $C_k$ and $N$ is the total number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE Parameter Estimation Steps\n",
    "\n",
    "For simplification, let's consider the covariance matrix $\\Sigma_k$ for $k=1,2$ to be **isotropic** matrices, that is, the covariance matrix is diagonal and the element along the diagonal is the same, or: $\\Sigma_k = \\sigma_k^2\\mathbf{I}$.\n",
    "\n",
    "* What are the parameters? The mean and covariance of the Gaussian distribution for both classes.\n",
    "\n",
    "Given the assumption of the Gaussian form, how would you estimate the parameters for $p(x|C_1)$ and $p(x|C_2)$? We can use **maximum likelihood estimate** for the mean and covariance, because we are looking for the parameters of the distributions that *maximize* the data likelihood!\n",
    "\n",
    "The MLE estimate for the mean of class $C_k$ is:\n",
    "\n",
    "$$\\mathbf{\\mu}_{k,\\text{MLE}} = \\frac{1}{N_k}\\sum_{n\\in C_k} \\mathbf{x}_n$$\n",
    "\n",
    "where $N_k$ is the number of training data points that belong to class $C_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming the classes follow a (bivariate or 2-D) Gaussian distribution and, for simplicity, let's assume the covariance matrices are **isotropic**, that is, $\\Sigma_k = \\sigma^2_k \\mathbf{I}$.\n",
    "\n",
    "The MLE steps for parameter estimation are:\n",
    "\n",
    "1. Write down the observed data likelihood, $\\mathcal{L}^0$\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L}^0 &= P(x_1,x_2,\\dots,x_N|C_k)\\\\\n",
    "&= \\prod_{n=1}^N P(x_n|C_k),\\text{ data samples are i.i.d.} \\\\\n",
    "&= \\prod_{n=1}^N \\frac{1}{(2\\pi)^{1/2} |\\Sigma|^{1/2}} \\exp\\left\\{-\\frac{1}{2}(x_n-\\mu_k)^T\\Sigma_k^{-1}(x_n-\\mu_k)\\right\\}\\\\\n",
    "&= \\prod_{n=1}^N \\frac{1}{(2\\pi)^{1/2} |\\sigma_k^2 \\mathbf{I}|^{1/2}} \\exp\\left\\{-\\frac{1}{2\\sigma_k^2}(x_n-\\mu_k)^T\\mathbf{I}(x_n-\\mu_k)\\right\\}\\\\\n",
    "&= \\prod_{n=1}^N \\frac{1}{(2\\pi)^{1/2} (\\sigma_k^2)^{1/2}} \\exp\\left\\{-\\frac{1}{2\\sigma_k^2}(x_n-\\mu_k)^T(x_n-\\mu_k)\\right\\}\n",
    "\\end{align}\n",
    "\n",
    "2. Take the log-likelihood, $\\mathbf{L}$. This *trick* helps in taking derivatives.\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L} &= \\ln\\left(\\mathcal{L}^0\\right) \\\\\n",
    "&= \\sum_{n=1}^N -\\frac{1}{2}\\ln 2\\pi - \\frac{1}{2}\\ln\\sigma_k^2 - \\frac{1}{2\\sigma_k^2}(x_n-\\mu_k)^T(x_n-\\mu_k)\n",
    "\\end{align}\n",
    "\n",
    "3. Take the derivative of the log-likelihood function with respect to the parameters of interest. For Gaussian distribution they are the mean and covariance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mu_k} &= 0\\\\\n",
    "\\sum_{n\\in C_k} \\frac{1}{\\sigma_k^2} (x_n - \\mu_k) &= 0\\\\\n",
    "\\sum_{n\\in C_k} (x_n - \\mu_k) &= 0 \\\\\n",
    "\\sum_{n\\in C_k} x_n - \\sum_{n\\in C_k} \\mu_k &= 0 \\\\\n",
    "\\sum_{n\\in C_k} x_n - N_k \\mu_k &= 0 \\\\\n",
    "\\mu_k & = \\frac{1}{N_k} \\sum_{n\\in C_k} x_n\n",
    "\\end{align}\n",
    "\n",
    "This is the sample mean for each class. And,\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\sigma_k^2} &= 0\\\\\n",
    "\\sum_{n\\in C_k} -\\frac{1}{2\\sigma_k^2} + \\frac{2(x_n - \\mu_k)^T(x_n - \\mu_k)}{(2\\sigma_k^2)^2} &=0 \\\\\n",
    "\\sum_{n\\in C_k} -1 + \\frac{(x_n - \\mu_k)^T(x_n - \\mu_k)}{\\sigma_k^2} &=0 \\\\\n",
    "\\sum_{n\\in C_k} \\frac{(x_n - \\mu_k)^T(x_n - \\mu_k)}{\\sigma_k^2} &=N_k \\\\\n",
    "\\sigma_k^2 &= \\sum_{n\\in C_k} \\frac{(x_n - \\mu_k)^T(x_n - \\mu_k)}{N_k}\n",
    "\\end{align}\n",
    "\n",
    "This is the sample variance for each class. Then we can create $\\Sigma_k = \\sigma_k^2 \\mathbf{I}$, which is the (biased) sample covariance for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, if we want to estimate an entire covariance matrix, we would have to take the derivative of the log-likelihood function with respect to every entry in the covariance matrix.\n",
    "\n",
    "We can determine the values for $p(C_1)$ and $p(C_2)$ from the number of data points in each class:\n",
    "\n",
    "$$p(C_k) = \\frac{N_k}{N}$$\n",
    "\n",
    "where $N$ is the total number of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the mean and covariance for each class from the training data\n",
    "mu1 = np.mean(data_C1, axis=0)\n",
    "print('Mean of Class 1: ', mu1)\n",
    "\n",
    "cov1 = np.cov(data_C1.T)\n",
    "print('Covariance of Class 1: ',cov1)\n",
    "\n",
    "mu2 = np.mean(data_C2, axis=0)\n",
    "print('Mean of Class 2: ',mu2)\n",
    "\n",
    "cov2 = np.cov(data_C2.T)\n",
    "print('Covariance of Class 2: ',cov2)\n",
    "\n",
    "# Estimate the prior for each class\n",
    "pC1 = data_C1.shape[0]/(data_C1.shape[0] + data_C2.shape[0])\n",
    "print('Probability of  Class 1: ',pC1)\n",
    "\n",
    "pC2 = data_C2.shape[0]/(data_C1.shape[0] + data_C2.shape[0])\n",
    "print('Probability of Class 2: ',pC2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute a grid of values for x and y \n",
    "x = np.linspace(-4, 4, 100)\n",
    "y = np.linspace(-4, 4, 100)\n",
    "xm, ym = np.meshgrid(x, y)\n",
    "X = np.flip(np.dstack([xm,ym]),axis=0)\n",
    "\n",
    "# Let's plot the probabaility density function (pdf) for each class\n",
    "\n",
    "# TO BE FINISHED IN CLASS\n",
    "\n",
    "fig =plt.figure(figsize=(15,5))\n",
    "fig.add_subplot(1,2,1)\n",
    "plt.imshow(y1, extent=[-4,4,-4,4])\n",
    "plt.colorbar()\n",
    "plt.xlabel('Feature 1'); plt.ylabel('Feature 2')\n",
    "plt.title('PDF Likelihood for Class 1')\n",
    "\n",
    "fig.add_subplot(1,2,2)\n",
    "plt.imshow(y2, extent=[-4,4,-4,4])\n",
    "plt.colorbar()\n",
    "plt.xlabel('Feature 1'); plt.ylabel('Feature 2')\n",
    "plt.title('PDF Likelihood for Class 2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig =plt.figure(figsize=(15,5))\n",
    "fig.add_subplot(1,2,1)\n",
    "plt.scatter(data_C1[:,0], data_C1[:,1], c='r',alpha=0.3)\n",
    "plt.imshow(y1, extent=[-4,4,-4,4],cmap='YlOrRd')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Feature 1'); plt.ylabel('Feature 2')\n",
    "plt.title('PDF Likelihood for Class 1')\n",
    "\n",
    "fig.add_subplot(1,2,2)\n",
    "plt.scatter(data_C2[:,0], data_C2[:,1], c='b',alpha=0.3)\n",
    "plt.imshow(y2, extent=[-4,4,-4,4], cmap='Blues')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Feature 1'); plt.ylabel('Feature 2')\n",
    "plt.title('PDF Likelihood for Class 2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the posterior distributions: they represent our classification decision\n",
    "\n",
    "# TO BE FINISHED IN CLASS\n",
    "\n",
    "\n",
    "fig =plt.figure(figsize=(15,5))\n",
    "fig.add_subplot(1,2,1)\n",
    "plt.imshow(pos1, extent=[-4,4,-4,4])\n",
    "plt.colorbar()\n",
    "plt.xlabel('Feature 1'); plt.ylabel('Feature 2')\n",
    "plt.title('Posterior for Class 1')\n",
    "\n",
    "fig.add_subplot(1,2,2)\n",
    "plt.imshow(pos2, extent=[-4,4,-4,4])\n",
    "plt.colorbar()\n",
    "plt.xlabel('Feature 1'); plt.ylabel('Feature 2')\n",
    "plt.title('Posterior for Class 2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the decision boundary:\n",
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "# TO BE FINISHED IN CLASS\n",
    "\n",
    "plt.colorbar()\n",
    "plt.xlabel('Feature 1'); plt.ylabel('Feature 2')\n",
    "plt.title('Region to Decide Class 1');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's use this classifier to predict the class label for point $[1,1]$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1,1]\n",
    "\n",
    "# TO BE FINISHED IN CLASS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What about $x=[2,4]$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
