{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "egyptian-brief",
   "metadata": {},
   "source": [
    "# Midterm Exam: Thursday, October 7\n",
    "\n",
    "Please check your email to find the time you are scheduled to take the exam.\n",
    "\n",
    "**All exam will be proctored with Honorlock.**\n",
    "\n",
    "## <font color=blue>Allowed Material:</font>\n",
    "\n",
    "1. **Formula sheet**: 1-page letter-sized of **formulas** (front and back, handwritten or typed)\n",
    "    * Other than labeling your equations, **DO NOT** include *any* definitions, pseudo-code or any algorithm description\n",
    "    * You must submit your formula sheet along with your handwritten solutions\n",
    "2. **Calculator**: scientific calculator\n",
    "3. **Writing paper**: 5 sheets (letter-sized) will suffice\n",
    "    * Some questions will be typed directly in the Canvas quiz, others will be solved on paper\n",
    "    * During the Honorlock 360 room scan, please show to the camera the clear writing sheets of paper and your formula sheet\n",
    "4. **Additional Electronic Device**: please have your phone, tablet or other device available with the [CamScanner](https://www.camscanner.com/) app installed. \n",
    "    * This device is only to be used at the end of the exam to take pictures of your handwritten solutions AND formula sheet\n",
    "    * You will be able to open a second tab to download the files\n",
    "    \n",
    "\n",
    "## <font color=orange>TOTAL TIME:</font> 2 hours + 15 minutes\n",
    "\n",
    "**<font color=red>Communications between students or anyone else is considered cheating. Turn off all Slack notifications and other communications channels!</font>**\n",
    " \n",
    "Ten (10) minutes prior to the time you are scheduled to take the exam, you will be able to see the Midterm Exam quiz in Canvas.\n",
    "\n",
    "Make sure your testing environment is comfortable, and start whenever you are ready."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-graphic",
   "metadata": {},
   "source": [
    "# Lecture 18 - Midterm Exam Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifteen-thing",
   "metadata": {},
   "source": [
    "## Midterm Exam Coverage\n",
    "\n",
    "The midterm exam will cover all materials from Lecture 1-17. These include:\n",
    "\n",
    "1. **Introduction to Machine Learning <font color=blue>(Lectures 1-3)</font>**\n",
    "    * Definition of Machine Learning, Artificial Intelligene and Deep Learning\n",
    "    * Types of learning in Machine Learning\n",
    "    * Supervised Learning diagram\n",
    "    * (Linear) Regression\n",
    "    * Performance Metrics for regression: error metrics, Q-Q plot, hypothesis test, confidence intervals\n",
    "    \n",
    "2. **Experimental Design and Analysis <font color=blue>(Lectures 4-9)</font>**\n",
    "    * Feature representation: polynomial basis function, radial basis function, etc.\n",
    "    * Model selection\n",
    "    * Occam's Razor\n",
    "    * Generalization\n",
    "    * Regularization: ridge and lasso\n",
    "    * Cross-Validation\n",
    "    * The No Free Lunch Theorem\n",
    "    * The Bias-Variance Trade-Off\n",
    "    * Experimental Design\n",
    "    * Hyperparameters tuning\n",
    "    * The Curse of Dimensionality\n",
    "    \n",
    "3. **Bayesian Learning <font color=blue>(Lectures 10-13)</font>**\n",
    "    * Frequentist vs Bayesian statistics\n",
    "    * Bayesian interpretation of Regression Least Squares Objective Function \n",
    "    * Maximum Likelihood Estimation (MLE)\n",
    "    * Maximum A Posteriori (MAP)\n",
    "    * Bayesian Prior Equivalence\n",
    "    * Conjugate Priors, Online update\n",
    "    \n",
    "4. **Generative Classification <font color=blue>(Lectures 14-17)</font>**\n",
    "    * Distinction between classification and regression\n",
    "    * Probabilistic Generative Models\n",
    "    * Naive Bayes Classifier\n",
    "    * Expectation-Maximization Algorithm\n",
    "    * Gaussian Mixture Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-force",
   "metadata": {},
   "source": [
    "## How to prepare for exam\n",
    "\n",
    "**This is a suggestion only.**\n",
    "\n",
    "1. Review/read all Notebooks.\n",
    "\n",
    "2. Create your formula sheet. **Do not include pseudo-code or solutions to examples in homework assignments.** \n",
    "\n",
    "3. Review/redo exercises from Part 1 of HW1 and HW2. \n",
    "\n",
    "4. Review/redo exercises from SA1, SA2, SA3 and discussion boards 1 and 2.\n",
    "\n",
    "5. Solve practice exam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-experiment",
   "metadata": {},
   "source": [
    "## Discussion Board Questions\n",
    "\n",
    "Thank you for posting your questions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-unemployment",
   "metadata": {},
   "source": [
    "## Post 1\n",
    "\n",
    "*I understand all the concepts conceptually but am currently finding myself struggling when it comes to deriving equations such as finding Bayesian interpretation of functions (much like in the HW2). If you could go over general strategies with some examples on how to approach these problem types, I would appreciate it!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "restricted-upper",
   "metadata": {},
   "source": [
    "Let's review the exercise we did in lecture 11 and 12. \n",
    "\n",
    "Consider the experiment where we flip *a* coin. We are interested in estimating the probability of flipping heads as new samples from this experiment arrive.\n",
    "\n",
    "Let heads=1 and tails=0, so our sample space is $S=\\{1,0\\}$. We can model the data likelihood as a Bernoulli distribution:\n",
    "\n",
    "$$P(x|\\mu) = \\mu^x(1-\\mu)^{1-x} = \\begin{cases}\\mu, & x=1 \\\\ 1-\\mu, & x=0 \\\\ 0, & \\text{otherwise}  \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-harrison",
   "metadata": {},
   "source": [
    "Further assume that we have *prior knowledge* that the unknown parameter we are trying to estimate ($\\mu \\equiv$ probability of flipping heads) is a random variable and we will assume that it follows a Beta distribution:\n",
    "\n",
    "$$\\text{Beta}(\\mu|\\alpha,\\beta) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\mu^{\\alpha-1} (1-\\mu)^{\\beta-1}$$\n",
    "\n",
    "where $\\Gamma(x) = (x-1)!$ and $\\alpha,\\beta>0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-pharmaceutical",
   "metadata": {},
   "source": [
    "We can now solve for the parameter $\\mu$ using the MAP approach:\n",
    "\n",
    "$$\\arg_{\\mu} \\max P(\\mu|X) = \\arg_{\\mu}\\max \\ln P(\\mu|X)$$\n",
    "\n",
    "Let $\\mathcal{L} = \\ln P(\\mu|X)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-amsterdam",
   "metadata": {},
   "source": [
    "We write the posterior probability as:\n",
    "\n",
    "\\begin{align}\n",
    "P(\\mu|X) &= \\frac{P(X|\\mu)P(\\mu)}{P(X)}\\\\\n",
    "&\\propto P(X|\\mu)P(\\mu)\\\\\n",
    "&\\propto \\left(\\prod_{n=1}^N \\mu^{x_n} (1-\\mu)^{1-x_n}\\right) \\mu^{\\alpha-1} (1-\\mu)^{\\beta-1} \\\\\n",
    "&= \\mu^m (1-\\mu)^l \\mu^{\\alpha-1} (1-\\mu)^{\\beta-1} \\\\\n",
    "&= \\mu^{m+\\alpha-1} (1-\\mu)^{l+\\beta-1}\n",
    "\\end{align}\n",
    "\n",
    "where $m$ the number of heads, $l$ the number of tails, and $N=m+l$ the total number of coin flips.\n",
    "\n",
    "* This defines a Conjugate Prior relationship as the prior and the posterior follow the same shape.\n",
    "\n",
    "* In an online environment (i.e. as new samples are coming), we can update the prior with the posterior distribution. By inspecting the posterior result, we see that:\n",
    "    * $\\alpha \\leftarrow \\alpha+m$ and $\\beta = \\beta + l$.\n",
    "    * We \"replace the prior with the posterior\" by updating the parameters of the prior distributions with those of the new posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-being",
   "metadata": {},
   "source": [
    "Then\n",
    "\\begin{align*}\n",
    "\\mathcal{L} &= \\ln P(\\mu|X)\\\\\n",
    "&\\propto \\ln\\left(\\mu^{m+\\alpha-1} (1-\\mu)^{l+\\beta-1}\\right)\\\\\n",
    "&= (m+\\alpha-1)\\ln(\\mu) + (l+\\beta-1)\\ln(1-\\mu)\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-state",
   "metadata": {},
   "source": [
    "We can now *optimize* our posterior probability:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial  \\ln \\left( P(\\mu|E) \\right)}{\\partial \\mu} &= 0\\\\\n",
    "\\frac{m+\\alpha-1}{\\mu} + \\frac{l+\\beta-1}{1-\\mu} &= 0\\\\\n",
    "\\mu &= \\frac{m+\\alpha-1}{m + l + \\alpha + \\beta -2}\n",
    "\\end{align}\n",
    "\n",
    "This is our estimation of the probability of heads using MAP!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-modeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import beta\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-computer",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trueMU = 0.5 # 0.5 for a fair coin\n",
    "a = 1\n",
    "b = 2\n",
    "\n",
    "x = np.linspace(-0.1,1.1,100)\n",
    "xr = range(-1,3)\n",
    "plt.plot(x, stats.beta(a,b).pdf(x))\n",
    "plt.xlabel('$\\mu$'); plt.ylabel('P($\\mu$)'); plt.title('Initial Prior')\n",
    "plt.show()\n",
    "\n",
    "print('Alpha = ', a)\n",
    "print('Beta = ', b)\n",
    "\n",
    "Outcomes=[]\n",
    "for N in range(1,16):\n",
    "    Outcomes += [stats.bernoulli(trueMU).rvs(size=1)[0]]\n",
    "    estimate_mu = (np.sum(Outcomes)+a-1)/(len(Outcomes)+a+b-2)\n",
    "    \n",
    "    # Data Likelihood:\n",
    "    fig=plt.figure(figsize=(15,5))\n",
    "    fig.add_subplot(1,2,1)\n",
    "    plt.stem(xr, stats.bernoulli(np.sum(Outcomes)/len(Outcomes)).pmf(xr))\n",
    "    plt.xlabel('$x$'); plt.ylabel('P(X|$\\mu$)'); \n",
    "    plt.title('Data Likelihood, '+str(N)+' samples')\n",
    "    \n",
    "    # Posterior/Prior:\n",
    "    fig.add_subplot(1,2,2)\n",
    "    plt.plot(x, stats.beta(a,b).pdf(x))\n",
    "    plt.xlabel('$\\mu$'); plt.ylabel('P($\\mu|$alpha, beta)'); \n",
    "    plt.title('Posterior/Prior: alpha='+str(a)+', beta='+str(b))\n",
    "    plt.show()\n",
    "    \n",
    "    # Update Prior distribution\n",
    "    a += np.sum(Outcomes)\n",
    "    b += len(Outcomes)-np.sum(Outcomes)\n",
    "    \n",
    "    # Print estimate for mu\n",
    "    print('Number of samples: ', len(Outcomes))\n",
    "    print('Data: ',Outcomes)\n",
    "    print('MAP estimate mu = ', estimate_mu)\n",
    "    print('New alpha = ', a)\n",
    "    print('New beta = ', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-grocery",
   "metadata": {},
   "source": [
    "## Another Example: Exercise 7 from Practice Midterm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-barbados",
   "metadata": {},
   "source": [
    "Given $N$ data points, $\\mathbf{X}=\\{x_i\\}_{i=1}^N$, all non-negative (i.e. $x_i\\geq 0$). Assume the input samples $\\mathbf{X}$ are independent and identically distributed (i.i.d.), and each sample comes from an Exponential distribution\n",
    "\n",
    "$$p(x|\\lambda) = \\begin{cases}\\lambda e^{-\\lambda x} & x\\geq 0\\\\0 & x<0 \\end{cases}$$\n",
    "\n",
    "with a Gamma prior distribution on the rate parameter, $\\lambda$,\n",
    "\n",
    "$$p(\\lambda|\\alpha,\\beta) = \\frac{1}{\\Gamma(\\alpha)} \\beta^{\\alpha}\\lambda^{\\alpha-1}e^{-\\beta\\lambda}$$\n",
    "\n",
    "Using clean paper, answer the following questions:\n",
    "\n",
    "1. (5 points) Derive the maximum likelihood estimate (MLE) for the rate parameter $\\lambda$.\n",
    "\n",
    "2. (5 points) Derive the maximum a posteriori (MAP) estimate for the rate parameter $\\lambda$.\n",
    "\n",
    "3. (5 points) Is the Gamma distribution a conjugate prior for the rate parameter $\\lambda$ of the exponential distribution? Why or why not?\n",
    "\n",
    "4. (5 points) Suppose you would like to update the Gamma prior distribution and your MAP estimate of $\\lambda$ in an online fashion, as you obtain more data. Using precise language (namely, pseudo-code and the equations you derived in part (2)), how would you update the prior distribution and your estimate of $\\lambda$ in an online fashion?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-grain",
   "metadata": {},
   "source": [
    "1. Let's define the observed data likelihood:\n",
    "\n",
    "$$\\mathcal{L}^0 = \\prod_{i=1}^N p(x_i|\\lambda) = \\prod_{i=1}^N \\lambda e^{-\\lambda x_i} = \\lambda^N e^{-\\lambda\\sum_{i=1}^N x_i}$$\n",
    "\n",
    "Applying the log-function:\n",
    "\n",
    "$$\\mathcal{L} = \\ln \\mathcal{L}^0 = N \\ln\\lambda -\\lambda \\sum_{i=1}^N x_i$$\n",
    "\n",
    "Solving for $\\lambda$:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\lambda} &= 0\\\\\n",
    "\\frac{N}{\\lambda} - \\sum_{i=1}^N x_i &= 0\\\\\n",
    "\\lambda_{\\text{MLE}} &= \\frac{N}{\\sum_{i=1}^N x_i}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-width",
   "metadata": {},
   "source": [
    "2. Let's define the observed data likelihood:\n",
    "\n",
    "$$\\mathcal{L}^0 = \\prod_{i=1}^N p(x_i|\\lambda)p(\\lambda|\\alpha,\\beta) = \\left(\\prod_{i=1}^N \\lambda e^{-\\lambda x_i}\\right)\\frac{1}{\\Gamma(\\alpha)} \\beta^{\\alpha}\\lambda^{\\alpha-1}e^{-\\beta\\lambda} = \\frac{1}{\\Gamma(\\alpha)} \\beta^{\\alpha}\\lambda^{N+\\alpha-1}e^{-\\lambda\\left(\\beta + \\sum_{i=1}^N x_i\\right)}$$\n",
    "\n",
    "Applying the log-function:\n",
    "\n",
    "$$\\mathcal{L} = \\ln \\mathcal{L}^0 = -ln \\Gamma(\\alpha) + \\alpha\\ln\\beta + (N+\\alpha-1)\\ln\\lambda -\\lambda \\left(\\beta + \\sum_{i=1}^N x_i\\right)$$\n",
    "\n",
    "Solving for $\\lambda$:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\lambda} &= 0\\\\\n",
    "\\frac{N+\\alpha-1}{\\lambda} - \\beta \\sum_{i=1}^N x_i &=0\\\\\n",
    "\\lambda_{\\text{MAP}} &= \\frac{N+\\alpha-1}{\\beta +\\sum_{i=1}^N x_i}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-education",
   "metadata": {},
   "source": [
    "3. Yes, because the posterior probability (defined as $\\mathcal{L}^0$ in 2) is proportional to an exponential distribution. We say that the prior-posterior have a conjugate prior relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceramic-preparation",
   "metadata": {},
   "source": [
    "4. The pseudo-code is as follows:\n",
    "    \n",
    "    1. Initialize the parameters of the prior ditribution, $\\alpha$ and $\\beta$.\n",
    "        * Compute the prior probability $p(\\lambda|\\alpha,\\beta)$.\n",
    "    2. As new data arrives to $\\mathbf{X}$, do:\n",
    "        * Compute the data likelihood $P(\\mathbf{X}|\\lambda)$.\n",
    "        * Compute the posterior probability $P(\\lambda|\\mathbf{X})\\propto P(\\mathbf{X}|\\lambda)P(\\lambda|\\alpha,\\beta)$\n",
    "    3. Update the prior distribution with the posterior distribution\n",
    "        * That is, replace the parameters $\\alpha$ and $\\beta$ with those estimated for the posterior\n",
    "        * This will produce a new informed prior distribution.\n",
    "    4. Go back to step B.\n",
    "    \n",
    "    \n",
    "$$\\lambda_{\\text{MAP}} = \\frac{N+\\alpha-1}{\\beta +\\sum_{i=1}^N x_i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remarkable-count",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.special import gamma\n",
    "trueLAM = 2\n",
    "a = 100\n",
    "b = 10\n",
    "\n",
    "x = np.linspace(0,20,100)\n",
    "xd = np.linspace(0,3,100)\n",
    "plt.plot(x, stats.gamma(a,scale=1/b).pdf(x))\n",
    "plt.xlabel('$\\lambda$'); plt.ylabel('P($\\lambda$)'); plt.title('Initial Prior')\n",
    "plt.show()\n",
    "\n",
    "print('Alpha = ', a)\n",
    "print('Beta = ', b)\n",
    "\n",
    "Outcomes=[]\n",
    "for i in range(1,31):\n",
    "    Outcomes += [stats.expon(scale=1/trueLAM).rvs(size=1)[0]]\n",
    "    estimate_lam = (len(Outcomes) + a - 1)/(b + np.sum(Outcomes))\n",
    "    \n",
    "    # Data Likelihood:\n",
    "    fig=plt.figure(figsize=(15,5))\n",
    "    fig.add_subplot(1,2,1)\n",
    "    plt.plot(xd, stats.expon(scale=1/estimate_lam).pdf(xd))\n",
    "    plt.xlabel('$x$'); plt.ylabel('P(X|$\\lambda$)'); \n",
    "    plt.title('Data Likelihood, '+str(len(Outcomes))+' samples')\n",
    "    \n",
    "    # Posterior/Prior:\n",
    "    fig.add_subplot(1,2,2)\n",
    "    plt.plot(x, stats.gamma(a,scale=1/b).pdf(x))\n",
    "    plt.xlabel('$\\lambda$'); plt.ylabel('P($\\lambda|$alpha, beta)'); \n",
    "    plt.title('Posterior/Prior: alpha='+str(a)+', beta='+str(b))\n",
    "    plt.show()\n",
    "    \n",
    "    # Update Prior distribution\n",
    "    a += len(Outcomes)\n",
    "    b += np.sum(Outcomes)\n",
    "    \n",
    "    # Print estimate for mu\n",
    "    print('Number of samples: ', len(Outcomes))\n",
    "#     print('Data: ',Outcomes)\n",
    "    print('MAP estimate LAMBDA = ', estimate_lam)\n",
    "    print('New alpha = ', a)\n",
    "    print('New beta = ', b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-network",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
