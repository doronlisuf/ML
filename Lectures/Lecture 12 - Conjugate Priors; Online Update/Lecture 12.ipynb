{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 12 - Conjugate Priors; Online Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "from scipy.stats import multivariate_normal\n",
    "import textwrap\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trueMU = 0.5 # 0.5 for a fair coin\n",
    "Nflips = 10\n",
    "a = 2\n",
    "b = 10\n",
    "x = np.arange(0,1,0.0001)\n",
    "Beta = (math.gamma(a+b)/(math.gamma(a)*math.gamma(b)))*x**(a-1)*(1-x)**(b-1)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(x, Beta, label='Beta Distribution, B('+str(a)+','+str(b)+')')\n",
    "plt.legend(loc='best',fontsize=15)\n",
    "plt.xlabel('$\\mu$\\n(Probability of Heads)',size=15)\n",
    "plt.ylabel('$P(\\mu)$',size=15); \n",
    "plt.title('Prior Probability', size=15)\n",
    "plt.show();\n",
    "\n",
    "Outcomes = []\n",
    "for i in range(Nflips):\n",
    "    Outcomes += [stats.bernoulli(trueMU).rvs(1)[0]]\n",
    "    print(Outcomes)\n",
    "    print('MLE aka Frequentist Probability of Heads = ', np.sum(Outcomes)/len(Outcomes))\n",
    "    print('MAP aka Bayesian Probability of Heads = ', (np.sum(Outcomes)+a-1)/(len(Outcomes)+a+b-2))\n",
    "    input('Press enter to flip the coin again...\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<h2 align=\"center\"><span style=\"color:blue\">Maximum Likelihood Estimation (MLE)</span></h2>\n",
    "<center>(Frequentist approach)</center>\n",
    "\n",
    "$$\\arg_{\\mathbf{w}} \\max P(\\mathbf{t}|\\mathbf{w})$$\n",
    "\n",
    "In **Maximum Likelihood Estimation** we *find the set of parameters* that **maximize** the data likelihood $P(\\mathbf{t}|\\mathbf{w})$. We find the *optimal* set of parameters under some assumed distribution such that the data is most likely.\n",
    "\n",
    "* MLE focuses on maximizing the data likelihood, which *usually* provides a pretty good estimate\n",
    "\n",
    "* A common trick to maximize the data likelihood is to maximize the log likelihood\n",
    "\n",
    "* MLE is purely data driven \n",
    "\n",
    "* MLE works best when we have lots and lots of data\n",
    "\n",
    "* MLE will likely overfit when we have small amounts of data or, at least, becomes unreliable\n",
    "\n",
    "* It estimates relative frequency for our model parameters. Therefore it needs incredibly large amounts of data (infinite!) to estimate the true likelihood parameters\n",
    "    * This is a problem when we want to make inferences and/or predictions outside the range of what the training data has learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<h2 align=\"center\"><span style=\"color:orange\">Maximum A Posteriori (MAP)</span></h2>\n",
    "<center>(Bayesian approach)</center>\n",
    "\n",
    "\\begin{align}\n",
    "& \\arg_{\\mathbf{w}} \\max P(\\mathbf{t}|\\mathbf{w})P(\\mathbf{w}) \\\\ \n",
    "& \\propto \\arg_{\\mathbf{w}} \\max P(\\mathbf{w}|\\mathbf{t})\n",
    "\\end{align}\n",
    "\n",
    "In **Maximum A Posteriori** we *find the set of parameters* that **maximize** the the posterior probability $P(\\mathbf{w}|\\mathbf{t})$. We find the *optimal* set of parameters under some assumed distribution such that the parameters are most likely to have been drawn off of.\n",
    "\n",
    "* MAP focuses on maximizing the posterior probability - data  likelihood with a prior\n",
    "\n",
    "* A common trick to maximize the posterior probability is to maximize the log likelihood\n",
    "\n",
    "* MAP is data driven \n",
    "\n",
    "* MAP is mostly driven by the prior beliefs\n",
    "\n",
    "* MAP works great with small amounts of data *if* our prior was chosen well\n",
    "\n",
    "* We need to assume and select a distribution for our prior beliefs\n",
    "    * A wrong choice of prior distribution can impact negatively our model estimation\n",
    "    \n",
    "* When we have lots and lots of data, the data likelihood will take over and the posterior will depend less and less on the prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conjugate Priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If the posterior probability and the prior probability have the same (parametric) form, they are said to have a **conjugate prior** relationship.\n",
    "\n",
    "    * For example, consider the data likelihood $P(\\mathbf{t}|\\mu) \\sim \\mathcal{N}(\\mu, \\sigma^2)$ and the prior distribution $P(\\mu) \\sim \\mathcal{N}(\\mu_0,\\sigma_0^2)$. The posterior probability will also be Gaussian distributed\n",
    "    \n",
    "$$P(\\mu|\\mathbf{t}) \\sim \\mathcal{N}\\left(\\frac{\\sum_{i=1}^N x_i\\sigma_0^2 + \\mu_0\\sigma^2}{N\\sigma_0^2+\\sigma^2},\\left(\\frac{N}{\\sigma^2}+\\frac{1}{\\sigma_0^2}\\right)^{-1}\\right)$$\n",
    "\n",
    "<!--Proof: \n",
    "\n",
    "\\begin{align}\n",
    "P(\\mu|X) &\\propto P(X|\\mu)P(\\mu) \\\\\n",
    "& = \\prod_{i=1}^N \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{1}{2}\\frac{(x_i-\\mu)^2}{\\sigma^2}\\right)\\frac{1}{\\sqrt{2\\pi\\sigma_0^2}}\\exp\\left(-\\frac{1}{2}\\frac{(\\mu-\\mu_0)^2}{\\sigma_0^2}\\right) \\\\\n",
    "&=  \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\frac{1}{\\sqrt{2\\pi\\sigma_0^2}}\\exp\\left(\\sum_{i=1}^N \\left(-\\frac{1}{2}\\frac{(x_i-\\mu)^2}{\\sigma^2}\\right) -\\frac{1}{2}\\frac{(\\mu-\\mu_0)^2}{\\sigma_0^2} \\right) \\\\\n",
    "&=  \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\frac{1}{\\sqrt{2\\pi\\sigma_0^2}}\\exp\\left(-\\frac{1}{2}\\left(\\sum_{i=1}^N \\frac{(x_i-\\mu)^2}{\\sigma^2} +\\frac{(\\mu-\\mu_0)^2}{\\sigma_0^2} \\right) \\right) \\\\\n",
    "&=  \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\frac{1}{\\sqrt{2\\pi\\sigma_0^2}}\\exp\\left(-\\frac{1}{2}\\left(\\frac{\\sum_{i=1}^N x_i^2-2\\sum_{i=1}^N x_i\\mu +\\mu^2N)}{\\sigma^2} +\\frac{\\mu^2-2\\mu\\mu_0 +\\mu_0^2}{\\sigma_0^2} \\right) \\right) \\\\\n",
    "&=  \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\frac{1}{\\sqrt{2\\pi\\sigma_0^2}}\\exp\\left(-\\frac{1}{2} \\left(\\frac{N}{\\sigma^2}+\\frac{1}{\\sigma_0^2}\\right) - 2\\mu \\left( \\frac{\\sum_{i=1}^N x_i}{\\sigma^2} + \\frac{\\mu_0}{\\sigma_0^2}\\right) + \\frac{\\sum_{i=1}^N x_i^2}{\\sigma^2} + \\frac{\\mu_0^2}{\\sigma_0^2}\\right) \\\\\n",
    "&=  \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\frac{1}{\\sqrt{2\\pi\\sigma_0^2}}\\exp\\left(-\\frac{1}{2} \\left(\\frac{N}{\\sigma^2}+\\frac{1}{\\sigma_0^2}\\right) \\left(\\mu^2 - 2\\mu\\left(\\frac{\\sum_{i=1}^N x_i}{\\sigma^2} + \\frac{\\mu_0}{\\sigma_0^2}\\right)\\left(\\frac{N}{\\sigma^2}+\\frac{1}{\\sigma_0^2}\\right)^{-1} \\right) \\right) \\exp\\left(\\frac{\\sum_{i=1}^N x_i}{\\sigma^2} + \\frac{\\mu_0^2}{\\sigma_0^2}\\right)\n",
    "\\end{align} -->\n",
    "\n",
    "* There are many conjugate prior relationships, e.g., Bernoulli-Beta, Gausian-Gaussian, Gaussian-Inverse Wishart, Multinomial-Dirichlet.\n",
    "\n",
    "Conjugate prior relationships play an important role for **online updating** of the prior distribution.\n",
    "\n",
    "* In an online model estimation scenario, where the posterior has the same form as the prior, we can use the posterior as our new prior. This new prior is now **data-informed** and will update its parameters based on (1) our initial choice, and (2) the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjugate Prior Relationship: Gaussian-Gaussian\n",
    "\n",
    "For a D-dimensional Gaussian data likelihood with mean $\\mu$ and covariance $\\beta\\mathbf{I}$ and a prior distribution with mean $\\mu_0$ and covariance $\\Sigma_0$\n",
    "\n",
    "\\begin{align}\n",
    "P(\\mathbf{t}|\\mathbf{w}) &\\sim \\mathcal{N}(\\mathbf{\\mu}, \\beta\\mathbf{I}) \\\\\n",
    "P(\\mathbf{w}) &\\sim \\mathcal{N}(\\mathbf{\\mu}_0,\\Sigma_0)\n",
    "\\end{align}\n",
    "\n",
    "The posterior distribution\n",
    "\n",
    "\\begin{align}\n",
    "P(\\mathbf{w}|\\mathbf{t}) &\\sim \\mathcal{N}\\left(\\mathbf{\\mu}_N, \\Sigma_N\\right) \\\\\n",
    "\\mathbf{\\mu}_N &= \\Sigma_N \\left(\\Sigma_0^{-1}\\mathbf{\\mu}_0+\\beta\\mathbf{\\mathbf{X}}^T\\mathbf{t}\\right)\\\\\n",
    "\\Sigma_N^{-1} &= \\Sigma_0^{-1} + \\beta \\mathbf{\\mathbf{X}}^T\\mathbf{\\mathbf{X}}\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathbf{X}$ is the feature matrix of size $N \\times M$.\n",
    "\n",
    "* What happens with different values of $\\beta$ and $\\Sigma_0$?\n",
    "\n",
    "To simplify, let's assume the covariance of prior is **isotropic**, that is, it is a diagonal matrix with the same value along the diagonal, $\\Sigma_0 = \\alpha^{-1}\\mathbf{I}$. And, let also $\\mathbf{\\mu}_0 = [0,0]$, thus \n",
    "\n",
    "$$\\mu_N = \\beta \\Sigma_N\\mathbf{X}^T\\mathbf{t}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\Sigma_N = \\left(\\alpha^{-1}\\mathbf{I} + \\beta \\mathbf{X}^T\\mathbf{X}\\right)^{-1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Online Updating of the Prior using Conjugate Priors (Gaussian-Gaussian)\n",
    "\n",
    "Let's consider the example presented in the Bishop textbook (Figure 3.7 in page 155).\n",
    "\n",
    "Consider a single input variable $\\mathbf{x}$, a single target variable $\\mathbf{t}$ and a linear model of the form $y(\\mathbf{x},\\mathbf{w}) = w_0 + w_1\\mathbf{x}$.\n",
    "Because this has just two parameters coefficients, $w=[w_0, w_1]^T$, we can plot the prior and posterior distributions directly in parameter space (2-dimensional parameter space)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "Let's generate some synthetic data from the function $f(x, a) = w_0 + w_1x$ with parameter values $w_0 = −0.3$ and $w_1 = 0.5$ by first choosing values of $x_n$ from the uniform distribution $U(x_n|−1, 1)$, then evaluating $f(x_n, \\mathbf{w})$, and finally adding Gaussian noise with standard deviation of $\\sigma = 0.2$ to obtain the target values $t_n$.\n",
    "\n",
    "$$t_n = f(x_n, \\mathbf{w}) + \\epsilon = -0.3 + 0.5 x_n + \\mathbf{\\epsilon}$$\n",
    "\n",
    "where $\\mathbf{\\epsilon}\\sim \\mathcal{N}(0,\\beta\\mathbf{I})$.\n",
    "\n",
    "* **Our goal is to recover the values of $w_0$ and $w_1$ from such data, and we will explore the dependence on the size of the data set.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "For some data, $\\{x_n,t_n\\}_{n=1}^N$, we can pose this problem in terms of **Regularized Least Squares**:\n",
    "\n",
    "\\begin{align}\n",
    "J(\\mathbf{w}) &= \\frac{1}{2} \\sum_{n=1}^N \\left(t_n - y_n\\right)^2 + \\frac{\\lambda}{2} \\sum_{i=0}^1 w_i^2 \\\\\n",
    "&= \\frac{1}{N} \\sum_{n=1}^2 \\left(t_n - y_n\\right)^2 + \\frac{\\lambda}{2} \\left(w_0^2 + w_1^2\\right)\\\\\n",
    "& \\Rightarrow \\arg_{\\mathbf{w}}\\min J(\\mathbf{w})\n",
    "\\end{align}\n",
    "\n",
    "* Using **MAP**, we can rewrite our objective using the **Bayesian interpretation**:\n",
    "\n",
    "\\begin{align}\n",
    "\\arg_{\\mathbf{w}} \\max P(\\mathbf{\\epsilon}|\\mathbf{w})P(\\mathbf{w})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the data likelihood, $P(\\mathbf{\\epsilon}|\\mathbf{w})$, to be a Gaussian distribution with mean $\\mu = 0$ and variance $\\sigma^2 = \\beta\\mathbf{I}$. And let's also consider the prior distribution, $P(\\mathbf{w})$, to be a Gaussian distribution with mean $\\mu_0$ and variance $\\sigma_0^2 = \\alpha^{-1}\\mathbf{I}$. Then, using the derivations from above, we can rewrite our optimization as:\n",
    "\n",
    "\\begin{align}\n",
    "\\arg_{\\mathbf{w}} \\max & \\mathcal{N}(\\mathbf{\\epsilon}|0,\\beta\\mathbf{I})\\mathcal{N}(\\mathbf{w}|\\mathbf{\\mu_0} ,\\alpha^{-1}\\mathbf{I}) \\\\\n",
    "\\propto\\arg_{\\mathbf{w}} \\max & \\mathcal{N}\\left( \\beta \\Sigma_N^{-1} \\mathbf{X}^T\\mathbf{t}, \\Sigma_N \\right)\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathbf{\\mu}_0 = [0,0]$, $\\mathbf{X}$ is the polynomial feature matrix, and $\\Sigma_N = \\left(\\alpha^{-1}\\mathbf{I} + \\beta \\mathbf{X}^T \\mathbf{X}\\right)^{-1}$ is the covariance matrix of the posterior distribution.\n",
    "\n",
    "Note that we **do not known** the parameters of the prior distribution ($\\mu_0$ and $\\sigma_0$ are unknown). The parameters of the prior distribution will have to be chosen by the user. And they will essentially *encode* any behavior or a priori knowledge we may have about the weights.\n",
    "\n",
    "* **Both our data likelihood and prior distributions are in a 2-dimensional space (this is because our *model order* is $M=2$ -- we have 2 parameters!).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to generate data from $t = -0.3 + 0.5x + \\epsilon$ where $\\epsilon$ is drawn from a zero-mean Gaussin distribution.\n",
    "\n",
    "* **The goal is to estimate the values $w_0=-0.3$ and $w_1=0.5$**\n",
    "* The feature matrix $\\mathbf{X}$ can be computed using the polynomial basis functions\n",
    "* **Parameters to choose:** $\\beta$ and $\\alpha$\n",
    "\n",
    "We want to implement this scenario for a case that we are getting more data every minute. As we get more and more data, we want to **update our prior distribution using our posterior distribution (informative prior)**, because they take the have the same distribution form. This is only possible because because Gaussian-Gaussian have a conjugate prior relationship. That is, the posterior distribution is also a Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_prior_func(beta = 2, alpha = 1, draw_num=(0,1,10,20,50,100)):\n",
    "    '''Online Update of the Posterior distribution for a Gaussian-Gaussian conjugate prior.\n",
    "    Parameter:\n",
    "    beta - variance of the data likelihood (of the additive noise)\n",
    "    alpha - precision value or 1/variance of the prior distribution\n",
    "    draw_num - number of points collected at each instance.\n",
    "    \n",
    "    This function will update the prior distribution as new data points are received.\n",
    "    The prior distribution will be the posterior distribution from the last iteration.'''\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 20), dpi= 80, facecolor='w', edgecolor='k')\n",
    "\n",
    "    # true (unknown) weights\n",
    "    a = -0.3 # w0\n",
    "    b = 0.5  # w1\n",
    "    \n",
    "    # set up input space\n",
    "    rangeX = [-2, 2] # range of values for the input\n",
    "    step = 0.025 # distance between points\n",
    "    X = np.mgrid[rangeX[0]:rangeX[1]:step] # creates a grid of values for input samples\n",
    "\n",
    "    #initialize prior/posterior and sample data\n",
    "    S0 = (1/alpha)*np.eye(2) # prior covariance matrix\n",
    "    sigma = S0 # copying it so we can update it later\n",
    "    mean = [0,0] # mean for prior\n",
    "    \n",
    "    # Draws samples from Uniform(-1,1) distribution\n",
    "    draws = np.random.uniform(rangeX[0],rangeX[1],size=draw_num[-1])\n",
    "    # Generate the noisy target samples\n",
    "    T = a + b*draws + np.random.normal(loc=0, scale=math.sqrt(beta))\n",
    "\n",
    "    for i in range(len(draw_num)):\n",
    "        if draw_num[i]>0: #skip first image\n",
    "            \n",
    "            # INPUT DATA\n",
    "            #Feature Matrix (Polynomial features with M=2)\n",
    "            FeatureMatrix = np.array([draws[:draw_num[i]]**m for m in range(2)]).T\n",
    "            #Target Values\n",
    "            t = T[0:draw_num[i]]\n",
    "            \n",
    "            # POSTERIOR PROBABILITY\n",
    "            # Covariance matrix\n",
    "            sigma = np.linalg.inv(S0 + beta*FeatureMatrix.T@FeatureMatrix)\n",
    "            # Mean vector\n",
    "            mean = beta*sigma@FeatureMatrix.T@t\n",
    "            \n",
    "            # PARAMETER SPACE\n",
    "            # create a meshgrid of possible values for w's\n",
    "            w0, w1 = np.mgrid[rangeX[0]:rangeX[1]:step, rangeX[0]:rangeX[1]:step]\n",
    "            \n",
    "            # Define the Gaussian distribution for data likelihood\n",
    "            p = multivariate_normal(mean=t[draw_num[i]-1], cov=beta)\n",
    "            # Initialize the PDF for data likelihood\n",
    "            out = np.empty(w0.shape)\n",
    "            # For each value (w0,w1), compute the PDF for all data samples\n",
    "            for j in range(len(w0)):\n",
    "                out[j] = p.pdf(w0[j]+w1[j]*draws[draw_num[i]-1])\n",
    "            \n",
    "            # Plot the data likelihood\n",
    "            ax = fig.add_subplot(*[len(draw_num),3,(i)*3+1])\n",
    "            ax.pcolor(w0, w1, out)\n",
    "            # Add the current value for parameters w=(w0,w1)\n",
    "            ax.scatter(a,b, c='c')\n",
    "            myTitle = 'data likelihood'\n",
    "            ax.set_title(\"\\n\".join(textwrap.wrap(myTitle, 100)))\n",
    "\n",
    "        # PARAMETER SPACE\n",
    "        # create a meshgrid of possible values for w's\n",
    "        w0, w1 = np.mgrid[rangeX[0]:rangeX[1]:step, rangeX[0]:rangeX[1]:step]\n",
    "        \n",
    "        # POSTERIOR PROBABILITY\n",
    "        # initialize the matrix with posterior PDF values\n",
    "        pos = np.empty(w1.shape + (2,))\n",
    "        # for w0\n",
    "        pos[:, :, 0] = w0\n",
    "        # and for w1\n",
    "        pos[:, :, 1] = w1\n",
    "        # compute the PDF\n",
    "        p = multivariate_normal(mean=mean, cov=sigma)\n",
    "\n",
    "        #Show prior/posterior\n",
    "        ax = fig.add_subplot(*[len(draw_num),3,(i)*3+2])\n",
    "        ax.pcolor(w0, w1, p.pdf(pos))\n",
    "        # Add the value for parameters w=(w0,w1) that MAXIMIZE THE POSTERIOR\n",
    "        ax.scatter(a,b, c='c')\n",
    "        myTitle = 'Prior/Posterior'\n",
    "        ax.set_title(\"\\n\".join(textwrap.wrap(myTitle, 100)))\n",
    "\n",
    "        # DATA SPACE\n",
    "        ax = fig.add_subplot(*[len(draw_num),3,(i)*3+3])\n",
    "        for j in range(6):\n",
    "            # draw sample from the prior probability to generate possible values for parameters \n",
    "            w0, w1 = np.random.multivariate_normal(mean=mean, cov=sigma)\n",
    "            # Estimated labels\n",
    "            t = w0 + w1*X\n",
    "            # Show data space\n",
    "            ax.plot(X,t)\n",
    "            if draw_num[i] > 0:\n",
    "                ax.scatter(FeatureMatrix[:,1], T[0:draw_num[i]])\n",
    "            myTitle = 'data space'\n",
    "            ax.set_title(\"\\n\".join(textwrap.wrap(myTitle, 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Parameters\n",
    "# beta - variance of the data likelihood (for the additive noise)\n",
    "# alpha - precision value or 1/variance for the prior distribution\n",
    "# draw_num - number of points collected at each instance\n",
    "\n",
    "likelihood_prior_func(beta = 0.2, alpha = 2, draw_num=(0,1,2,20,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
